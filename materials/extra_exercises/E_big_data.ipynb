{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cd56076",
   "metadata": {},
   "source": [
    "## Working with large-scale image data\n",
    "\n",
    "Welcome to the session on handling large-scale image data. \n",
    "\n",
    "In modern microscopy, it is not uncommon to work with datasets that are gigabytes or even terabytes in size. Far too large to fit into your computer's RAM.\n",
    "\n",
    "This notebook will introduce 2 topics related to this challenge:\n",
    "\n",
    "1. **Dask** — a package for for lazy, parallel processing  \n",
    "2. **Big image data formats** — like NGFF (OME-Zarr), and how they integrate with Dask  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9126768c",
   "metadata": {},
   "source": [
    "### Dask\n",
    "\n",
    "A standard TIFF file is a single, monolithic block. To access even a small part of a 100 GB TIFF, a program often needs to read it whole, which is slow, memory-intensive or even impossible.\n",
    "\n",
    "Dask lets you work with arrays and data larger than memory via chunked, lazily evaluated operations.\n",
    "\n",
    "You can think of it as splitting a big image into small tiles (chunks), operating on each tile, and only loading the data or performing the computation when explicitly asked to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b345182",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import numpy as np\n",
    "\n",
    "# Create a large random “image” (but lazily)\n",
    "large = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# Show its structure - this is a dask array\n",
    "large\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22acb9f0",
   "metadata": {},
   "source": [
    "No actual data was generated yet. The `large` object is just a blueprint.\n",
    "\n",
    "Dask stores the instructions (the \"recipe\") to generate each chunk. It builds a **task graph**—a step-by-step plan of what needs to be done.\n",
    "\n",
    "We can add more instructions to this recipe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e935d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple operations, NumPy functions — sum, mean, etc.\n",
    "mean = np.mean(large)\n",
    "\n",
    "print(mean)  # This is still a Dask object, not the final number."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11dbb29",
   "metadata": {},
   "source": [
    "The calculation is only triggered when you explicitly ask for the result by calling the `.compute()` method.\n",
    "\n",
    "When `.compute()` is called, Dask's scheduler executes the task graph. It intelligently loads only the necessary chunks into memory at any given time and runs computations in parallel on your multi-core CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ed2b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# This is the memory-efficient way to compute a result.\n",
    "# Dask calculates the mean of each chunk, then combines those results.\n",
    "# The full array is never loaded into memory at once.\n",
    "result = mean.compute()  # This triggers the computation\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1bafbf",
   "metadata": {},
   "source": [
    "If we call `.compute()` on the *entire Dask array*, Dask will generate all chunks and assemble the full NumPy array in memory. This will fail if the array is larger than your RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866f82ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Dask generates all chunks and stitches them together to one big NumPy array in memory.\n",
    "# It does however utilize parallel processing\n",
    "result = large.compute()\n",
    "print(np.mean(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72095b7",
   "metadata": {},
   "source": [
    "Dask with real image I/O\n",
    "\n",
    "We can use a library like `dask-image` or `tifffile` library, which has special integration with Zarr, a format Dask understands natively, to read an image as Dask array.\n",
    "\n",
    "```python\n",
    "# An example using dask-image\n",
    "import dask_image.imread as di\n",
    "img = di.imread('../data/TEM.tiff', chunkshape=(512, 512))\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e834a9d1",
   "metadata": {},
   "source": [
    "Tifffile with `aszarr = True`\n",
    "\n",
    "**Zarr** is a format for storing N-dimensional arrays in chunks. Instead of one giant file, a Zarr store is a directory containing many small, independent files (the chunks). A program can read just the few chunks it needs for a specific task.\n",
    "\n",
    "The `tifffile` library can cleverly treat a standard TIFF file *as if* it were a Zarr store. This lets us open a huge TIFF lazily and wrap it in a Dask array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b87897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile\n",
    "import dask.array as da\n",
    "\n",
    "# Lazily open a TIFF as a Zarr-backed Dask array\n",
    "# This operation is instantaneous \n",
    "# because no pixel data is actually read from disk yet.\n",
    "store = tifffile.imread('../data/TEM1.tiff', aszarr=True)\n",
    "\n",
    "# Wrap that store in Dask, specifying chunk size\n",
    "img = da.from_zarr(store, chunks=(500, 500))\n",
    "\n",
    "# We can access metadata-like properties without computation.\n",
    "print(f\"\\nShape: {img.shape}\")\n",
    "print(f\"Data type: {img.dtype}\")\n",
    "print(f\"Number of chunks: {img.numblocks}\")\n",
    "print(f\"Chunk size: {img.chunksize}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aa78cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('type: ',type(img))\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2a20c9",
   "metadata": {},
   "source": [
    "You convert a TIFF to a chunked format like Zarr for faster access in the future. This triggers Dask to read the TIFF chunk-by-chunk and write out the Zarr chunks in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd34b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Zarr and save (this will create a folder ../data/TEM.zarr)\n",
    "img.to_zarr('../data/TEM.zarr', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6f3728",
   "metadata": {},
   "source": [
    "Once saved as a proper Zarr store, we can open it again lazily. This is extremely fast.\n",
    "\n",
    "We use the `zarr` library for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1806bd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "store = zarr.open('../data/TEM.zarr', mode='r')\n",
    "img = da.from_zarr(store)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff7c10c",
   "metadata": {},
   "source": [
    "Trying to plot the image with matplotlib.pyplot will automatically trigger .compute()!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85256ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(img, cmap='gray') # ! this will compute the entire array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab64946",
   "metadata": {},
   "source": [
    "Instead, you can only access a piece of data for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "preview = img[:500, :500].compute()\n",
    "plt.imshow(preview, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdf4549",
   "metadata": {},
   "source": [
    "Functions like da.mean(), da.sum(), da.std(), da.max(), and basic arithmetic (+, *, /) can be applied directly to Dask arrays.\n",
    "\n",
    "Most functions from libraries like scikit-image, or your own custom functions are not Dask-aware. They can be used with help of `map_blocks()` or `map_overlap()`\n",
    "\n",
    "- `map_blocks()`\n",
    "    - Applies function independently to each chunk.\n",
    "    - No information is shared across chunk boundaries.\n",
    "- `map_overlap()`\n",
    "    - Allows chunks to “borrow” extra pixels from neighboring chunks (`depth`).\n",
    "    - After the operation, the extra borders are trimmed, so the output array has the same shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f4a70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import gaussian\n",
    "from dask.array import map_overlap, map_blocks\n",
    "\n",
    "# Apply a Gaussian filter lazily (no computation yet)\n",
    "smoothed = da.map_blocks(gaussian, img, sigma=2)\n",
    "\n",
    "# Alternative with overlap control \n",
    "# depth=5 means 5 pixels from each side are temporarily \n",
    "# added to the chunk for computation.\n",
    "smoothed_overlap = img.map_overlap(lambda arr: gaussian(arr, sigma=2), depth=5)\n",
    "\n",
    "def smooth_image_fun(arr):\n",
    "    smoothed = gaussian(arr, sigma=2)\n",
    "    return smoothed\n",
    "smoothed_overlap = map_overlap(smooth_image_fun, img, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597c84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute for one chunk and plot it\n",
    "region = smoothed_overlap[:500, :500].compute()\n",
    "\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(region, cmap='gray')\n",
    "plt.title(\"One chunk - smoothed\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running a Complex Workflow: Cellpose on a Dask Array\n",
    "\n",
    "from cellpose import models\n",
    "import dask\n",
    "dask.config.set(scheduler='threads') # Use a thread-based scheduler for this workflow\n",
    "\n",
    "# --- Setup for Cellpose ---\n",
    "# 1. Initialize the Cellpose model\n",
    "model = models.Cellpose(model_type='cyto3')\n",
    "\n",
    "# --- Define a function to run Cellpose on a single chunk ---\n",
    "def run_cellpose_on_chunk(image_chunk):\n",
    "    # The `model.eval` function returns the mask as the first output\n",
    "    mask, _, _, _ = model.eval(image_chunk, diameter=120)\n",
    "    return mask\n",
    "\n",
    "lazy_mask = da.map_overlap(\n",
    "    run_cellpose_on_chunk,\n",
    "    img,\n",
    "    depth=5, # The size of the halo/overlap on each side\n",
    "    boundary='reflect',\n",
    "    dtype=np.uint16 # Cellpose masks are integer labels\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41d05bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask = lazy_mask.compute()\n",
    "print(\"...Cellpose segmentation complete!\")\n",
    "\n",
    "print(\"Shape of the final mask:\", final_mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b994765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualize a small region of the result ---\n",
    "import napari\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(img, name='Original Large Image')\n",
    "viewer.add_labels(final_mask, name='Dask-Cellpose Segmentation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d35528d",
   "metadata": {},
   "source": [
    "### Big image data formats & integration with Dask\n",
    "\n",
    "NGFF (Next-Generation File Format) is a specification for storing large bioimaging data in a chunked, multiscale format.\n",
    "\n",
    "OME-Zarr is an implementation of NGFF that includes standardized metadata defined by the Open Microscopy Environment (OME) on top of Zarr.\n",
    "\n",
    "Resolution-levels (multi-scale): Zarr also supports storing multiple downsampled versions of the image. This allows a viewer like Napari to quickly display a low-resolution version when zoomed out and only load the high-resolution chunks for the specific region you zoom into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ad2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the OME-Zarr file stored in S3 (online storage)\n",
    "\n",
    "image_id = 6001247\n",
    "ENPOINT_URL = 'https://uk1s3.embassy.ebi.ac.uk/'\n",
    "\n",
    "def load_binary_from_s3(name, resolution='0'):\n",
    "    root = '%s/%s/' % (name, resolution)\n",
    "    return da.from_zarr(ENPOINT_URL + root)\n",
    "\n",
    "name = 'idr/zarr/v0.1/%s.zarr' % image_id\n",
    "data = load_binary_from_s3(name)\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50d9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b0d59",
   "metadata": {},
   "source": [
    "Plot a single chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f96ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(data[0,1,100,:,:].compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45835bea",
   "metadata": {},
   "source": [
    "### --- Exercise ---\n",
    "\n",
    "Work with the multi-channel OME-zarr file from above.\n",
    "\n",
    "\n",
    "1. Load the OME-zarr image as dask-array (use same function), but change the resolution level (lower-resolution / level 1).\n",
    "2. Show the dask-array information. (run cell with the variable name)\n",
    "3. Work with the second channel only and take 20 consecutive middle slices from the z-stack\n",
    "4. Create a function that takes a 3D image chunk (Z, Y, X) and performs the following steps:\n",
    "    - Iterates through each 2D z-slice in the chunk.\n",
    "    - Applies a Gaussian filter (`sigma=2`) to the slice.\n",
    "    - Runs Cellpose `model.eval()` on the smoothed slice to get a labeled mask.\n",
    "    - Return labels stack\n",
    "5. Use `map_overlap() or map_blocks()` to apply your function to the sliced Dask array from step 3. You must specify parameter `dtype='int16'` for output labels\n",
    "6.  Call `.compute()` on the lazy result to trigger the full workflow and get the final segmentation mask as a NumPy array.\n",
    "7. Use `napari` to display the sliced image stack and overlay the computed segmentation labels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1478fb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cellpose import models\n",
    "model = models.Cellpose(model_type='nuclei')\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2475f2b",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# EXAMPLE SOLUTION \n",
    "\n",
    "# 1. Load the image (lower-resolution / level 1).\n",
    "res1_data = load_binary_from_s3(name, resolution=1)\n",
    "\n",
    "# 2. Get information - run this in separate cell\n",
    "res1_data\n",
    "\n",
    "# 3. Work with the second channel only and slice every 10th z-slice\n",
    "# We select T=0, C=1, and 20 Z slices.\n",
    "sliced_data = res1_data[0, 1, 120:140, :, :]\n",
    "print(f\"Step 3: Sliced data to shape {sliced_data.shape}\")\n",
    "\n",
    "# 4. Create function for applying smoothing and Cellpose segmentation\n",
    "def smooth_and_segment(arr):\n",
    "    labels_stack = []\n",
    "    for z_slice in arr:\n",
    "        sm = gaussian(z_slice, sigma=2)\n",
    "        labels, _, _, _ = model.eval(sm)\n",
    "        labels_stack.append(labels)\n",
    "    return np.stack(labels_stack)\n",
    "\n",
    "# 5. Apply function on sliced array inside map_overlap() function\n",
    "lazy_labels = da.map_overlap(\n",
    "    smooth_and_segment,\n",
    "    sliced_data,\n",
    "    dtype='int16'\n",
    ")\n",
    "\n",
    "# 6. Call .compute() on the lazy result to get the final NumPy array.\n",
    "final_labels = lazy_labels.compute()\n",
    "print(f\"Step 6: Computation complete. Final mask shape: {final_labels.shape}, dtype: {final_labels.dtype}\")\n",
    "\n",
    "# 7. Visualize sliced stack with labels with Napari.\n",
    "import napari\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(sliced_data.compute(), name='Sliced image')\n",
    "viewer.add_labels(final_labels, name='Cellpose labels')\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papi-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
