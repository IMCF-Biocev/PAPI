{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb5add9",
   "metadata": {},
   "source": [
    "## Image Analysis - DataFrames and Automation\n",
    "\n",
    "In this notebook, we will explore how to use **pandas** for working with tabular data, perform basic statistical analysis with **scipy**, and get familiar with another Python plotting library — **seaborn**.\n",
    "Finally, we will practice **automation on multiple files**, both for tabular data and for image processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22911561",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Lesson 1: DataFrames (pandas package)\n",
    "\n",
    "Pandas is a library used for data manipulation and analysis, particularly useful for dealing with structured data like tables. It provides the tools to create and work with DataFrames.\n",
    "\n",
    "DataFrame is a two-dimensional data structure similar to a spreadsheet (table).\n",
    "\n",
    "We typically import pandas like this:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create a DataFrame from different types of data.  \n",
    "\n",
    "# Create a DataFrame from a dictionary of lists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "measurement_data = {\n",
    "    'cell_id': [1, 2, 3, 4, 5],\n",
    "    'area': [150, 230, 95, 180, 40],\n",
    "    'intensity': [88.5, 95.2, 101.0, 89.7, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(measurement_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3746eb4",
   "metadata": {},
   "source": [
    "**nan** stands for “**Not a Number**\n",
    "\n",
    "It’s a special floating-point value used to represent missing or undefined numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096893c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily get statistics for our data with .describe()\n",
    "\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can extract a single column from a pandas DataFrame using its column name\n",
    "\n",
    "# This way it keeps the row index and behaves like a labeled one-dimensional array.\n",
    "area_column = df['area']\n",
    "print(area_column) \n",
    "print(type(area_column)) # pandas Series \n",
    "\n",
    "print()\n",
    "\n",
    "# This way it is  without the index or Series structure \n",
    "area_values = df['area'].values\n",
    "print(area_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter data based on conditions\n",
    "\n",
    "# This creates a new DataFrame containing only cells with area less than a value.\n",
    "filtered_df = df[df['area'] < 200]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e52073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining multiple logical conditions\n",
    "# Each condition must be in parentheses ( )\n",
    "filtered_df2 = df[(df['area'] < 200) & (df['area'] > 100)]\n",
    "filtered_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing (nan) values\n",
    "\n",
    "df_clean = df.dropna()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can sort dataframes by one or more columns.\n",
    "# ascending=True/False controls ascending/descending order\n",
    "\n",
    "sorted_df = df.sort_values(by='intensity', ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea64f7",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** ---\n",
    "\n",
    "You have experimental data in a dictionary. Your task is to analyze it with Pandas.\n",
    "1. Create a **Pandas DataFrame** from the given data dictionary.\n",
    "2. Filter the DataFrame to find all rows where the **Treatment** was `'Drug_A'`. \n",
    "  *Hint: use the equality operator `==`*\n",
    "3. From the filtered data, compute the **average Score** for Drug A.\n",
    "   *Hint: you can use .mean() on extracted 'Score' column*\n",
    "4. Using the entire DataFrame, find out **which treatments were effective**.  \n",
    "   *Hint: you can use .unique() on pandas Series object (extracted Treatment) to find unique treatments*\n",
    "5. Display the **3 rows with the highest Score**.  \n",
    "   *Hint: use `.sort_values('Score', ascending=False).head(3)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59366ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cd\n",
    "data = {'Animal_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "        'Treatment': ['Drug_A', 'Drug_B', 'Placebo', 'Drug_A', 'Drug_B', 'Placebo', 'Drug_A', 'Drug_B', 'Drug_A', 'Drug_B'],\n",
    "        'Result': ['Effective', 'Not Effective', 'Not Effective', 'Effective', 'Effective', 'Not Effective', 'Effective', 'Effective', 'Effective', 'Not Effective'],\n",
    "        'Score': [50.3, 3.5, 10.1, 17.0, 93.3, 1.5, 99.9, 73.7, 69.2, 0.5]}\n",
    "\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810da2a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Create DataFrame from dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Filter rows where Treatment is 'Drug_A'\n",
    "drug_a_df = df[df['Treatment'] == 'Drug_A']\n",
    "print(\"Rows with Drug_A treatment:\")\n",
    "print(drug_a_df)\n",
    "\n",
    "# 3. Average Score for Drug_A\n",
    "avg_score_drug_a = drug_a_df['Score'].mean()\n",
    "print(\"\\nAverage Score for Drug_A:\", avg_score_drug_a)\n",
    "\n",
    "# 4. Which treatments were effective?\n",
    "effective_treatments = df[df['Result'] == 'Effective']['Treatment'].unique()\n",
    "print(\"\\nTreatments with at least one effective result:\", effective_treatments)\n",
    "\n",
    "# 5. Display 3 rows with the highest 'Score'\n",
    "top3_scores = df.sort_values('Score', ascending=False).head(3)\n",
    "print(\"\\nTop 3 rows by Score:\")\n",
    "print(top3_scores)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7f948",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf64d56",
   "metadata": {},
   "source": [
    "##### Reading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e537fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save our DataFrame as table (csv, excel, ...)\n",
    "sorted_df.to_csv('first_measurements.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42874ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can read tables from file into DataFrames\n",
    "loaded_df = pd.read_csv('first_measurements.csv') # check other file formats\n",
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3356cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames are convenient for plotting\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(loaded_df['area'], loaded_df['intensity'])\n",
    "plt.title('scatterplot')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Mean intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01400866",
   "metadata": {},
   "source": [
    "##### Aggregation\n",
    "\n",
    "When we have multiple DataFrames, we can combine them into single one.\n",
    "\n",
    "pd.concat() is a pandas function used to combine multiple DataFrames along a particular axis (rows or columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf19bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation (multiple tables)\n",
    "\n",
    "df_image1 = pd.read_csv(\"../data/cells_control.csv\")\n",
    "df_image2 = pd.read_csv(\"../data/cells_diseased.csv\")\n",
    "df_image3 = pd.read_csv(\"../data/cells_conditioned.csv\")\n",
    "\n",
    "# Merge dataframes vertically\n",
    "combined_df = pd.concat([df_image1, df_image2, df_image3], ignore_index=True)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a812a8a",
   "metadata": {},
   "source": [
    "##### DataFrame data types\n",
    "\n",
    "Not all columns have to store numerical data.\n",
    "\n",
    "Data types can be:\n",
    "- int64, float64 → numeric columns\n",
    "- object → usually strings (categories, text)\n",
    "- category → categorical data (memory-efficient, useful for grouping)\n",
    "\n",
    "\n",
    "You can quickly check the data types of each column using:\n",
    "```python\n",
    "print(df.dtypes)   # Shows the type of each column\n",
    "print(df.info())   # Shows summary including data types and non-null counts\n",
    "```\n",
    "\n",
    "You can convert string/object columns to categorical with method: `.astype(\"category\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"Group\"] = combined_df[\"Group\"].astype(\"category\")\n",
    "combined_df[\"State\"] = combined_df[\"State\"].astype(\"category\")\n",
    "\n",
    "print(combined_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a506f2",
   "metadata": {},
   "source": [
    "##### Group-level statistics\n",
    "\n",
    "When your dataset has **categorical variables** (like experimental groups, treatments, or conditions) and **numeric measurements**, it is often useful to calculate statistics **per group**.\n",
    "\n",
    "- `describe()` can give summary statistics for the **entire DataFrame**, but it does not separate by category\n",
    "- to get statistics per group, you can use `groupby()` combined with `agg()` (aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe method is applied on entire dataframe without disciminating categories\n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to extract statistics per group, we can use methods group_by() and agg()\n",
    "\n",
    "# !! agg() works only on numerical columns\n",
    "# we need to remove all other types first (except the ones we are grouping by)\n",
    "reduced_df = combined_df.drop(\"State\", axis=1) # this removes column 'State'\n",
    "\n",
    "# Now we group by column 'Group'\n",
    "summary_per_group = reduced_df.groupby('Group', observed=False).agg([\"count\", \"mean\", \"std\"]) \n",
    "summary_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use multiple factors\n",
    "summary_per_group_and_state = combined_df.groupby(['Group','State'], observed=False).agg([\"count\",\"sum\", \"mean\", \"std\"]) \n",
    "summary_per_group_and_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc4555",
   "metadata": {},
   "source": [
    "##### Visualizing group differences with Seaborn\n",
    "\n",
    "The **Seaborn** library builds on Matplotlib but provides higher-level plotting functions tailored for statistical data.\n",
    "\n",
    "A particularly useful feature is the `hue` argument, which colors data points by group/category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0227b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn \n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(\n",
    "    data=combined_df, \n",
    "    x=\"Group\", \n",
    "    y=\"Mean_Intensity\", \n",
    "    hue=\"Group\", \n",
    "    palette=\"Set2\" # color-coding\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f956db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some seaborn plots also allow discrimination of multiple factors\n",
    "  \n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(\n",
    "    data=combined_df, \n",
    "    x=\"Area\", y=\"Circularity\", \n",
    "    hue=\"Group\", style=\"State\"\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a86049",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing\n",
    "\n",
    "Visualizations help us see patterns and differences in data, but they are not enough for statistical confirmation.  \n",
    "To formally test whether groups differ, we use hypothesis testing.\n",
    "\n",
    "In Python, the `scipy.stats` module provides a variety of statistical tests.\n",
    "\n",
    "Below, we will use a **t-test** to compare whether the **means of two groups are equal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6212f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# the following is combination of filtering DataFrame and extracting column Mean_Intensity\n",
    "controls = combined_df[combined_df['Group']=='Control']['Mean_Intensity']\n",
    "diseased = combined_df[combined_df['Group']=='Diseased']['Mean_Intensity']\n",
    "\n",
    "# the t-test takes two lists/arrays \n",
    "# by default the ttest_ind() performs two-sided t-test and assumes equal variance\n",
    "t_stat, p_value = ttest_ind(controls, diseased)\n",
    "\n",
    "print(f\"\\nT-test p-value for comparing areas: {p_value:.9f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca48a6",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** ---\n",
    "\n",
    "\n",
    "On `combined_df` from previous exercises perform the following:\n",
    "\n",
    "1. Compute mean and standard deviation of all numerical columns per factor **State**. \n",
    "    - Which condition have larger average area?\n",
    "2. Make a boxplot comparing **Area** across categories of **State**.\n",
    "4. Add a swarmplot (`sns.swarmplot`) on top of boxplot to see individual datapoints.\n",
    "5. Perform a t-test comparing \"Circularity\" between Control and Diseased.\n",
    "6. Perform a t-test comparing \"Mean_Intensity\" between Starved Pre-conditioned and Unstarved Pre-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eced295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b2422",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# Compute mean and std per State\n",
    "r_df = combined_df.drop('Group', axis=1)\n",
    "summary_state = r_df.groupby(\"State\", observed=False).agg([\"mean\", \"std\"]).round(2) # optional rounding\n",
    "print(summary_state)\n",
    "\n",
    "# Boxplot with swarmplot for Area per State\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(data=combined_df, x=\"State\", y=\"Area\", palette=\"Set2\", hue='State')\n",
    "sns.swarmplot(data=combined_df, x=\"State\", y=\"Area\", color='black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# T-test Circularity: Control vs Diseased\n",
    "controls = combined_df[combined_df[\"Group\"] == \"Control\"][\"Circularity\"]\n",
    "diseased = combined_df[combined_df[\"Group\"] == \"Diseased\"][\"Circularity\"]\n",
    "t_stat, p_value = ttest_ind(controls, diseased)\n",
    "print(f\"T-test Circularity (Control vs Diseased): p = {p_value:.4f}\")\n",
    "\n",
    "# T-test Mean_Intensity: Starved vs Unstarved Pre-conditioned\n",
    "pre_starved = combined_df[\n",
    "    (combined_df[\"Group\"] == \"Pre-conditioned\") & (combined_df[\"State\"] == \"Starved\")\n",
    "][\"Mean_Intensity\"]\n",
    "pre_unstarved = combined_df[\n",
    "    (combined_df[\"Group\"] == \"Pre-conditioned\") & (combined_df[\"State\"] == \"Unstarved\")\n",
    "][\"Mean_Intensity\"]\n",
    "t_stat, p_value = ttest_ind(pre_starved, pre_unstarved)\n",
    "print(f\"T-test Mean_Intensity (Starved vs Unstarved, Pre-conditioned): p = {p_value:.4f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1298b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Lesson 2: Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64e28d",
   "metadata": {},
   "source": [
    "#### A Quick Guide to File Paths\n",
    "\n",
    "Python offers several ways to handle paths, each with its own strengths. All are usefull as they automatically use the correct path separator (`/` or `\\`).\n",
    "\n",
    "Packages: `os`, `pathlib`, and `glob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- os.path.join(): The safe way to build paths ---\n",
    "import os\n",
    "\n",
    "# This automatically uses the correct separator for your OS (`/` or `\\`)\n",
    "folder = \"data\"\n",
    "filename = \"image_01.tif\"\n",
    "correct_path = os.path.join(folder, \"subfolder\", filename)\n",
    "print(f\"OS-safe path: {correct_path}\")\n",
    "\n",
    "\n",
    "# Get directory and filename parts\n",
    "parent_dir = os.path.dirname(correct_path)\n",
    "base_name = os.path.basename(correct_path)\n",
    "grandparent_dir = os.path.dirname(parent_dir)\n",
    "\n",
    "print(f\"File name part: {base_name}\")\n",
    "print(f\"Parent directory part: {parent_dir}\")\n",
    "print(f\"Grandparent directory part: {grandparent_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- pathlib  ---\n",
    "from pathlib import Path\n",
    "\n",
    "# It uses objects and the `/` operator\n",
    "p = Path(\"data\")\n",
    "csv_file_path = '..' / p / \"cells_control.csv\"\n",
    "print(f\"\\nPathlib object: {csv_file_path}\")\n",
    "print(f\"Does this file exist? {csv_file_path.exists()}\")\n",
    "\n",
    "\n",
    "# --- Get parts of the path ---\n",
    "print(f\"Parent folder: {csv_file_path.parent}\")\n",
    "print(f\"Filename only: {csv_file_path.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557aa314",
   "metadata": {},
   "source": [
    "Creating new directories (folders) is possible with both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn’t exist yet\n",
    "\n",
    "# os\n",
    "os.makedirs('test_folder_os', exist_ok=True)\n",
    "\n",
    "# pathlib\n",
    "folder = Path(\"test_folder_pathlib\")\n",
    "folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f30db",
   "metadata": {},
   "source": [
    "##### Listing files in folder\n",
    "\n",
    "- os.listdir()\n",
    "- glob: finding files with wildcards \n",
    "    - glob.glob(folder/*.csv) or Path(\"folder\").glob(\"*.csv\")\n",
    "\n",
    "\n",
    "Wildcards are special characters that help you match patterns in file names — instead of typing exact file names.\n",
    "For example: * means any number of characters\n",
    "\n",
    "| Goal                | glob                                         | pathlib                       |\n",
    "| ------------------- | -------------------------------------------- | ----------------------------- |\n",
    "| Find only in folder | `glob.glob(\"folder/*.tif\")`                    | `Path(\"folder\").glob(\"*.tif\")`  |\n",
    "| Find recursively    | `glob.glob(\"folder/**/*.tif\", recursive=True)` | `Path(\"folder\").rglob(\"*.tif\")` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae75b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir() lists all files\n",
    "\n",
    "my_path = os.path.join('..', 'data')\n",
    "files = os.listdir(my_path)\n",
    "\n",
    "# Output is a list of filenames\n",
    "print(files[:10]) # print first 10 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension\n",
    "only_tif_files = [tif for tif in files if tif.endswith('.tif')]\n",
    "\n",
    "# the above is same as typing:\n",
    "only_tif_files = []\n",
    "for tif in files:\n",
    "    if tif.endswith('.tif'):\n",
    "        only_tif_files.append(tif)\n",
    "\n",
    "# print selection\n",
    "print(only_tif_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- glob.glob(): the way to find files with wildcards ---\n",
    "import glob\n",
    "\n",
    "# Find all files ending with .tif\n",
    "my_path = os.path.join('..', 'data')\n",
    "tif_files = glob.glob(os.path.join(my_path, \"*.tif\"))\n",
    "print(f\"\\nFound n TIF files: {len(tif_files)}\")\n",
    "\n",
    "# The output is a list of paths\n",
    "print(tif_files[-5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To search recursively (including subfolders)\n",
    "\n",
    "tif_files_recursive = glob.glob(os.path.join(r\"../data\", \"**\", \"*.tif\"), recursive=True)\n",
    "print(len(tif_files_recursive))\n",
    "print(tif_files_recursive[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc540c",
   "metadata": {},
   "source": [
    "##### --- **Exercise** ---\n",
    "\n",
    "1. **Create a path** to the `data` folder by combining the relative marker `'..'` and the folder name `'data'` - store the path in a variable `data_path`.\n",
    "   - Use either **`os.path.join()`** or **`pathlib.Path`**.\n",
    "2. **List all files** in your defined path and print how many files were found (len() function).  \n",
    "    *Hint*: Use os.listdir() or glob.glob(\"path+/*\")\n",
    "3. **List all images** in the folder (ending with **.tif** or **.png**).\n",
    "    *Hint*: you can combine previous output with `endswith()` function or combine glob searches (`glob.glob(\"folder/*.tif\") + glob.glob(\"folder/*.png\")`)\n",
    "4. **Find all .csv files** that include **'con'** in their name (e.g. control, conditioned)\n",
    "    - *Hint*: use `'substring' in 'string'` to find if `'con'` is in filename\n",
    "    - read them into a list a list of DataFrames - `pd.read_csv()`\n",
    "    - combine them into one DataFrame - `pd.concat(list_of_dfs, ignore_index=True)`\n",
    "    - save it to a new folder `results` by name `combined.csv` - `os.makedirs()` to create a folder and `df.to_csv()` to save DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6a0bc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "# EXAMPLE SOLUTION\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Create path to data folder ---\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "print(\"Data path:\", data_path)\n",
    "# or\n",
    "data_path2 = Path(\"..\") / \"data\"\n",
    "print(\"Data path:\", data_path2)\n",
    "\n",
    "# --- 2. List all files and count them ---\n",
    "all_files = glob.glob(os.path.join(data_path, \"*\"))\n",
    "print(f\"Found {len(all_files)} files:\")\n",
    "print(all_files)\n",
    "\n",
    "# --- 3. List all image files (.tif or .png) ---\n",
    "image_files = glob.glob(os.path.join(data_path, \"*.tif\")) + glob.glob(os.path.join(data_path, \"*.png\"))\n",
    "print(f\"\\nFound {len(image_files)} image files:\")\n",
    "print(image_files)\n",
    "\n",
    "# or list comprehension with tuple ('.tif', '.png')\n",
    "image_files2 = [file for file in all_files if file.endswith(('.tif', '.png'))]\n",
    "print(f\"\\nFound {len(image_files2)} image files.\")\n",
    "\n",
    "# --- 4. Find all CSVs with 'con' in name, combine, and save ---\n",
    "csv_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
    "csv_files_con = [f for f in csv_files if \"con\" in os.path.basename(f)]\n",
    "print(f\"\\nCSV files with 'con' in name: {csv_files_con}\")\n",
    "\n",
    "dfs = [pd.read_csv(f) for f in csv_files_con]\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Create results folder and save\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "combined_df.to_csv(\"results/combined.csv\", index=False)\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be987da",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Lesson 3: Image analysis workflow - recap\n",
    "\n",
    "In the introductury session we practiced designing and tuning a full image-processing workflow:\n",
    "- We loaded and visualized microscopy images.\n",
    "- We adjusted individual processing steps such as filtering, thresholding, and segmentation.\n",
    "- We extracted quantitative measurements (e.g., area, intensity, circularity).\n",
    "- We packaged this workflow in a function to be able to run it in a loop.\n",
    "\n",
    "- Now, we will recap this session\n",
    "- Add analysis of those measurements using **Pandas**, **Seaborn**, and **scipy** packages.\n",
    "- Practice automatic processing of batch of images or applying different settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da35d8",
   "metadata": {},
   "source": [
    "##### Image loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import imageio.v3 as iio\n",
    "\n",
    "input_image = io.imread('../data/noisy_cells.tif')\n",
    "input_image2 = iio.imread('../data/noisy_cells.tif')\n",
    "\n",
    "print('Image shape:', input_image.shape)\n",
    "print('Image shape:', input_image2.shape)\n",
    "\n",
    "print('Are images identical?', np.array_equal(input_image, input_image2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd032f",
   "metadata": {},
   "source": [
    "##### Image processing & segmentation\n",
    "\n",
    "Within `skimage` and `scipy`, there are many modules that provide functions for applying common processing operations on images or masks — such as filtering, thresholding, morphology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "gaussian_blur = filters.gaussian(input_image, sigma=2)         \n",
    "# Smooths image, reduces noise by averaging neighboring pixels\n",
    "\n",
    "median_filtered = filters.median(input_image)                  \n",
    "# Reduces noise while better preserving edges\n",
    "\n",
    "sobel_edges = filters.sobel(input_image)                       \n",
    "# Detects edges by computing the gradient magnitude of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8597fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(input_image, cmap='gray')\n",
    "ax[0].set_title('Original Image')\n",
    "\n",
    "ax[1].imshow(gaussian_blur, cmap='gray')\n",
    "ax[1].set_title('Gaussian Blur')\n",
    "\n",
    "ax[2].imshow(median_filtered, cmap='gray')\n",
    "ax[2].set_title('Median Filter')\n",
    "\n",
    "ax[3].imshow(sobel_edges, cmap='gray')\n",
    "ax[3].set_title('Sobel Edges')\n",
    "\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58997933",
   "metadata": {},
   "source": [
    "Segmentation is the process of separating an image into meaningful regions. In a simple case, we can segment image pixels into two classes (e.g., foreground vs. background) by applying a threshold on pixel values.\n",
    "\n",
    "The thresholded image produces a mask, which is a boolean array indicating which pixels belong to the foreground (True) and which to the background (False).\n",
    "\n",
    "\n",
    "The result of instance segmentation using `skimage.measure.label()` is called labels, which is an integer array where each connected object (e.g., a nucleus) is assigned a unique integer value, and the background is usually 0. This allows you to identify and analyze individual objects separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaeed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 30 # manual \n",
    "threshold = filters.threshold_mean(gaussian_blur) # automatic \n",
    "\n",
    "binary_mask = gaussian_blur > threshold\n",
    "print(binary_mask.dtype)\n",
    "\n",
    "plt.imshow(binary_mask, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "labels = measure.label(binary_mask)\n",
    "print(labels.dtype)\n",
    "plt.imshow(labels, cmap = 'nipy_spectral')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b603f706",
   "metadata": {},
   "source": [
    "##### Parameter Sweep\n",
    "\n",
    "When tuning a processing workflow, it is typical to test multiple parameters.\n",
    "\n",
    "Let’s compare how different levels of Gaussian blur affect the same image with help of a loop.\n",
    "\n",
    "- We define a list of parameter values (sigma_values) to test.\n",
    "- We use a simple **loop** to apply the same function with each parameter.\n",
    "- We use another **loop** to plot images with subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff04799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sigma values for Gaussian blur\n",
    "sigma_values = [1, 3, 5, 7, 9, 11]\n",
    "\n",
    "# Apply Gaussian filter with different sigmas\n",
    "blurred_images = []\n",
    "for sigma in sigma_values:\n",
    "    blurred = filters.gaussian(input_image, sigma=sigma)\n",
    "    blurred_images.append(blurred)\n",
    "\n",
    "# Alternative: list comprehension\n",
    "# blurred_images = [filters.gaussian(image, sigma=s) for s in sigma_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15327286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results side by side\n",
    "fig, axes = plt.subplots(1, len(sigma_values),figsize=(25, 5))\n",
    "\n",
    "# zip() is a built-in Python function \n",
    "# it lets you iterate over multiple sequences (lists, tuples...) in parallel\n",
    "for ax, s, img in zip(axes, sigma_values, blurred_images):\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_title(f\"σ = {s}\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.suptitle(\"Effect of Gaussian Blur with Different σ\", fontsize=16)\n",
    "plt.show()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60196b0",
   "metadata": {},
   "source": [
    "##### --- ***Mini Exercise*** ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55697f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [1,2,3,4]\n",
    "dictionary = {\n",
    "    'dogs': 2,\n",
    "    'cats': 9,\n",
    "    'sloths': 11,\n",
    "    'spiders': 999\n",
    "\n",
    "}\n",
    "\n",
    "# Work with me to use zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0fd532",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** ---\n",
    "\n",
    "Test how different thresholding algorithms segment the same image.\n",
    "\n",
    "- Use the image from previous exercise (`input_image` or `gaussian_blur`).\n",
    "- Create a dictionary of thresholding methods (Otsu, Li, Yen, Minimum).\n",
    "    - such as `methods = {'name1': function1(), 'name2': function2()}`\n",
    "- Create a new empty dictionary to store results.\n",
    "    - such as `new_dictionary = {}`\n",
    "- Use a loop to iterate over the methods in dictionary (*Hint: loop through the dictionary items using `for key, value in methods.items()`)\n",
    "    - apply thresholding method to get threshold value\n",
    "    - generate a binary mask with threshold value\n",
    "    - store binary array in the new dictionary under the method name (*Hint: store value under key with `new_dictionary[key]=value`*)\n",
    "- Optional: Plot binary arrays in one figure. (*Hint: you can iterate over axis and dict items together with `for ax, (key, value) in zip(axes, dict.items()`*)\n",
    "    - Optional: Plot the original image alongside binary arrays in one figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acae820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eda7ca7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# load thresholding functions\n",
    "from skimage.filters import threshold_otsu, threshold_li, threshold_yen, threshold_minimum\n",
    "\n",
    "# Dictionary of thresholding methods\n",
    "methods = {\n",
    "    \"Otsu\": threshold_otsu,\n",
    "    \"Li\": threshold_li,\n",
    "    \"Yen\": threshold_yen,\n",
    "    \"Min\": threshold_minimum\n",
    "}\n",
    "\n",
    "# Create empty dictionary to store masks\n",
    "masks = {}\n",
    "\n",
    "# Apply each thresholding method in a loop\n",
    "for name, func in methods.items():\n",
    "    thresh = func(input_image)        \n",
    "    mask = input_image > thresh      \n",
    "    masks[name] = mask          \n",
    "\n",
    "# Add original image to the new dictionary to make plotting easier\n",
    "masks['raw'] = input_image\n",
    "\n",
    "# Plot original image + threshold results\n",
    "fig, axes = plt.subplots(1, len(masks), figsize=(16, 5))\n",
    "\n",
    "for ax, (name, mask) in zip(axes, masks.items()):\n",
    "    ax.imshow(mask, cmap=\"gray\")\n",
    "    ax.set_title(name)\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9526c43e",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0e2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# By the way, skimage has a function to try all thresholds\n",
    "\n",
    "filters.try_all_threshold(input_image, figsize=(8, 5), verbose=False)\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44e471",
   "metadata": {},
   "source": [
    "##### Object measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3186f5",
   "metadata": {},
   "source": [
    "The `measure.regionprops_table()` function from scikit-image computes quantitative features for each labeled region in an image.  \n",
    "\n",
    "Each connected region (identified by a unique label) can be characterized by geometric and intensity-based properties.\n",
    "\n",
    "The output of this function is a is a dictionary containing one array per measured property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurements\n",
    " \n",
    "# We specify which properties we want to measure for each object.\n",
    "properties_to_measure = ('label', 'area', 'mean_intensity', 'perimeter', 'eccentricity')\n",
    "\n",
    "# regionprops_table uses our `labels` image and the original `nuclei_image`\n",
    "props_dict = measure.regionprops_table(\n",
    "    labels,\n",
    "    intensity_image=input_image,\n",
    "    properties=properties_to_measure\n",
    ")\n",
    "\n",
    "props_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary of results into a Pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "image_df = pd.DataFrame(props_dict)\n",
    "\n",
    "image_df.head(5) # show first 5 rows of a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5ff40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Process all images in folder\n",
    "\n",
    "Let's combine everything together.\n",
    "\n",
    "We will use for-loop to apply a user-defined function with processing and measurement steps on all images in folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae65055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define our final, reusable analysis function\n",
    "def analyze_image_final(image_array, source_filename):\n",
    "    blurred_image = filters.gaussian(image_array, 3)\n",
    "    threshold_value = filters.threshold_otsu(blurred_image)\n",
    "    mask = blurred_image > threshold_value\n",
    "    label_image = measure.label(mask)\n",
    "    props_dict = measure.regionprops_table(label_image, intensity_image=image_array,\n",
    "                                           properties=('label', 'area', 'mean_intensity'))\n",
    "    results_df = pd.DataFrame(props_dict)\n",
    "    # Add the source filename to track our data\n",
    "    results_df['source_file'] = source_filename\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb2eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Find the files\n",
    "input_folder = r\"../data/batch_analysis/input\"\n",
    "file_list = glob.glob(os.path.join(input_folder, \"*.png\"))\n",
    "\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Loop and aggregate results\n",
    "import imageio.v3 as iio\n",
    "\n",
    "all_image_results = []\n",
    "print(\"\\nStarting batch processing...\")\n",
    "\n",
    "# Iterate over list of filepaths\n",
    "for file_path in file_list:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # Read image\n",
    "    image = iio.imread(file_path)\n",
    "    \n",
    "    # Get just the filename from the full path for cleaner tables\n",
    "    filename_only = os.path.basename(file_path)\n",
    "    \n",
    "    # Apply our function - our function returns DataFrame with measurements\n",
    "    single_image_df = analyze_image_final(image, filename_only)\n",
    "\n",
    "    # Add DataFrame for currently processed file to a pre-defined list\n",
    "    all_image_results.append(single_image_df)\n",
    "    \n",
    "print()\n",
    "print('Number of dataframes in list:', len(all_image_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d096fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Concatenate into a final DataFrame\n",
    "final_batch_df = pd.concat(all_image_results, ignore_index=True)\n",
    "\n",
    "print(\"\\n--- Final Batch Results ---\")\n",
    "final_batch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f89f54",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** --- \n",
    "\n",
    "At the introductory session you were supposed to create an analyze nuclei function that takes in 2 arguments - a microscopic image (2D array) and its name (string). The function should identify individual nuclei, measure their properties (e.g., area and mean intensity), and return both a labeled image and a DataFrame containing the measurements along with a column storing the filename.\n",
    "\n",
    "The version of this function from example solution is defined below, use it to solve the following exercise. \n",
    "\n",
    "***Batch Process the Entire Folder***\n",
    "\n",
    "You have the tools, you have the function. Now it's time to automate everything!\n",
    "\n",
    "**Instructions:**\n",
    "1. Use glob to get a list of all `.tif` files in the `data/batch_analysis/nuclei_data` folder.\n",
    "2. Create an empty list called `all_nuclei_results`.\n",
    "3. Write a `for` loop that iterates through your list of file paths.\n",
    "4. Inside the loop:\n",
    "    - Load the current image using `iio.imread()`.\n",
    "    - Get just the filename (without the folder path) using `os.path.basename()`.\n",
    "    - Call your `analyze_nuclei` function with the loaded image and the filename.\n",
    "    - Append the DataFrame returned by the function to your `all_nuclei_results` list.\n",
    "5. After the loop, use `pd.concat()` to combine the list of DataFrames into a single, master DataFrame.\n",
    "6. Print the head and tail of your final DataFrame to see the combined results from all images.\n",
    "7. As a final analysis, create a boxplot showing the distribution of nuclei `area` for each of the images.\n",
    "    - *Hint*: `seaborn.boxplot` is great for this, you can use `source` column as `hue` argument. \n",
    "    - Usage: `sns.boxplot(data=dataframe, x=x_axis_column, y=y_axis_column, hue=group_column)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27037352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this code cell\n",
    "from skimage import filters, morphology, measure\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "def analyze_nuclei(image_array, source):\n",
    "    filtered_array = filters.gaussian(image_array, sigma=1)\n",
    "    threshold_value = filters.threshold_otsu(filtered_array)\n",
    "    mask = filtered_array > threshold_value\n",
    "    processed_mask = ndi.binary_fill_holes(mask)\n",
    "    mask_cleaned = morphology.remove_small_objects(processed_mask, min_size=50)\n",
    "    label_image = measure.label(mask_cleaned)\n",
    "\n",
    "    props = measure.regionprops_table(label_image, intensity_image=image_array,\n",
    "                                      properties=['label', 'area', 'mean_intensity'])\n",
    "    df = pd.DataFrame(props)\n",
    "\n",
    "    df['filename'] = source\n",
    "\n",
    "    return df, label_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ef074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376df0c3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Get list of all .tif files in the folder\n",
    "image_files = glob.glob(\"../data/batch_analysis/nuclei_data/*.tif\")\n",
    "\n",
    "# 2. Create an empty list to store results\n",
    "all_nuclei_results = []\n",
    "\n",
    "# 3. Loop through each file\n",
    "for file_path in image_files:\n",
    "    # Load image\n",
    "    image_array = iio.imread(file_path)\n",
    "    \n",
    "    # Extract filename only\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Analyze nuclei\n",
    "    df, labels = analyze_nuclei(image_array, filename)\n",
    "    \n",
    "    # Append results to list\n",
    "    all_nuclei_results.append(df)\n",
    "\n",
    "# 5. Combine all DataFrames into a single master DataFrame\n",
    "master_df = pd.concat(all_nuclei_results, ignore_index=True)\n",
    "\n",
    "# 6. Inspect results\n",
    "print(master_df.head())\n",
    "print(master_df.tail())\n",
    "\n",
    "# 7. Boxplot of nuclei area per image\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=master_df, x='filename', y='area', palette=\"Set3\", hue='filename')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Nuclei Area\")\n",
    "plt.xlabel(\"Source Image\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558e98d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### Slice-by slice reading\n",
    "\n",
    "When working with image data stored as a series of files, it is often useful to load all slices into a single multi-dimensional array for easier processing.\n",
    "\n",
    "The **scikit-image** function `imread_collection()` allows you to read multiple images at once using a filename pattern.\n",
    "You can use a wildcard (*) to match all files in a folder that belong to your dataset. This collection can then be converted into a **NumPy** array, effectively creating an image stack.\n",
    "\n",
    "However, the number of z-slices, channels or frames is not recognized. You have to reshape the loaded data into the appropriate multi-dimensional form (for example, (z, c, y, x)) your self.\n",
    "\n",
    "*Note*: Alternatively, you can build your own for-loops to load images from disk manually.\n",
    "This approach gives you more flexibility — for example, to sort slices and channels, skip specific files, or arrange the data into custom dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "im_collection = io.imread_collection('../data/batch_analysis/tiffs/' + \"*\")\n",
    "image_stack = im_collection.concatenate()\n",
    "image_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shape of array with reshape\n",
    "\n",
    "num_channels = 2\n",
    "num_z_slices = 5\n",
    "num_t_frames = 10\n",
    "image5d = np.reshape(image_stack, (num_t_frames, num_z_slices, num_channels, image_stack.shape[-2], image_stack.shape[-1]))\n",
    "image5d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import stackview\n",
    "stackview.slice(image5d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6a4b9",
   "metadata": {},
   "source": [
    "#### Pixel-Based Colocalization \n",
    "\n",
    "Pixel-based colocalization assesses the statistical correlation between the intensity values of corresponding pixels in two or more channels. It's important to remember that colocalization only indicates that two signals are present within the same volume resolved by the microscope - it does not prove molecular interaction.\n",
    "\n",
    "**The Cytofluorogram (Scatter Plot):**\n",
    "A 2D scatter plot is the primary visualization tool for colocalization. Each point on the plot represents a single pixel, with its x-coordinate being its intensity in Channel 1 and its y-coordinate its intensity in Channel 2.\n",
    "\n",
    "There are many types of coefficients you can evaluate (Pearson’s correlation, Spearman’s rank correlation, Manders’ overlap coefficient, cross-correlation analysis...). In this notebook we will compute the Pearson correlation coefficient.\n",
    "\n",
    "- **Pearson's Correlation Coefficient (PCC):** Measures the linear relationship between the two channels' intensities. It ranges from -1 (perfect anti-correlation) to +1 (perfect correlation), with 0 indicating no correlation. A major drawback is its sensitivity to background pixels, which often creates a strong, artificial positive correlation. Thresholding is essential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b1831",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup: Imports and Load a Multi-Channel Image ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import stackview\n",
    "from skimage.io import imread\n",
    "from skimage import filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_image = imread(r'../data/cellpainting.tif')\n",
    "\n",
    "print(raw_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stackview for interactive visualization of the channels\n",
    "\n",
    "stackview.switch(\n",
    "    {\"actin\":   raw_image[0,:,:],\n",
    "     \"er\":raw_image[1,:,:],\n",
    "     \"speckles\":      raw_image[2,:,:],\n",
    "     \"mito\":      raw_image[3,:,:],\n",
    "     \"nuclei\":      raw_image[4,:,:],\n",
    "    },\n",
    "    colormap=['pure_red',\"pure_green\", \"pure_yellow\", \"pure_magenta\", 'pure_cyan'],\n",
    "    toggleable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store channels in a dictionary for easy access\n",
    "\n",
    "channels_dict = {\n",
    "    \"actin\": raw_image[0,:,:],\n",
    "    \"er\": raw_image[1,:,:],\n",
    "    \"speckles\": raw_image[2,:,:],\n",
    "    \"mito\": raw_image[3,:,:],\n",
    "    \"nuclei\": raw_image[4,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688cbb2",
   "metadata": {},
   "source": [
    "Visualize Pixel Intensity Correlations with a cytofluorogram.\n",
    "\n",
    "Cytofluorogram is a 2D histogram (scatter-like plot) showing how pixel intensities in two channels correlate\n",
    "\n",
    "Let's create a cytofluorogram for the **mito** and **er** channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aedd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For efficient analysis, flatten the 2D image arrays into 1D arrays of pixels\n",
    "flattened_pixels = {key: value.ravel() for key, value in channels_dict.items()}\n",
    "df_pixels = pd.DataFrame(flattened_pixels)\n",
    "\n",
    "print(f\"Total number of pixels: {len(df_pixels)}\")\n",
    "df_pixels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45282eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The Cytofluorogram (Scatter Plot) ---\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x=df_pixels['mito'], y=df_pixels['er'], s=1, alpha=0.1)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b00d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 2D histogram is often more informative for dense data\n",
    "# Note the high density of points near the origin, representing background pixels.\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.histplot(df_pixels, x='mito', y='er', cbar=True, cmap='YlGnBu_r', bins=200)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5f79a",
   "metadata": {},
   "source": [
    "Calculating Pearson's Correlation Coefficient (PCC)\n",
    "\n",
    "First, we calculate PCC on the raw pixel data, which includes a vast number of dark background pixels. This will demonstrate how background skews the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pearson's Correlation ---\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pcc, p_value = pearsonr(df_pixels['mito'], df_pixels['er'])\n",
    "print(f\"Pearson's r (Mito vs. ER): {pcc:.4f}, p-value: {p_value:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca691b2c",
   "metadata": {},
   "source": [
    "Now, we'll apply a threshold to each channel to create masks that separate signal from background. By analyzing only the pixels within these masks, we get a more biologically meaningful correlation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Create a mask for significant pixels in each channel ---\n",
    "\n",
    "# We can use Otsu's method to automatically find a threshold\n",
    "thresh_er = filters.threshold_otsu(channels_dict['er'])\n",
    "print(f\"Otsu threshold for 'er': {thresh_er}\")\n",
    "\n",
    "\n",
    "# Or we might use a manual threshold based on visual inspection\n",
    "threshold = 2500\n",
    "mask_er = channels_dict['er'] > threshold\n",
    "mask_mito = channels_dict['mito'] > threshold\n",
    "\n",
    "# --- Step 2: Combine masks ---\n",
    "combined_mask = np.logical_and(mask_mito, mask_er)\n",
    "# or\n",
    "combined_mask = mask_mito & mask_er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 3: Visualize all masks side-by-side ---\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(mask_mito, cmap='gray')\n",
    "axes[0].set_title('Mito Mask')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask_er, cmap='gray')\n",
    "axes[1].set_title('ER Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(combined_mask, cmap='gray')\n",
    "axes[2].set_title('Combined Mask')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec4698",
   "metadata": {},
   "source": [
    "##### Operations between masks\n",
    "\n",
    "In image analysis, it is often useful to combine or compare different binary masks to extract specific regions of interest. \n",
    "\n",
    "Masks as boolean arrays (True for foreground, False for background), can be processed with logical operations to create new masks.\n",
    "\n",
    "Common operations:\n",
    "- *Inversion - NOT (`~`)* – flips all boolean values:\n",
    "    ```python\n",
    "    inverted_mask = ~mask\n",
    "    ```\n",
    "- *Intersection (`&`)* – keeps only pixels present in both masks:\n",
    "    ```python\n",
    "    overlap_mask = mask1 & mask2\n",
    "    ```\n",
    "- *Union (`|`)* – includes all pixels present in either mask:\n",
    "    ```python\n",
    "    combined_mask = mask1 | mask2\n",
    "    ```\n",
    "- *Difference / Subtraction (`& ~`)* – removes pixels of one mask from another:\n",
    "    ```python\n",
    "    cytoplasm_mask = cell_mask & ~nuclei_mask\n",
    "    ```\n",
    "- *Exclusive OR (`^`)* – pixels present in one mask or the other, but not both:\n",
    "    ```python\n",
    "    xor_mask = mask1 ^ mask2\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59d0f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the combined mask to both channels\n",
    "mito_masked = channels_dict['mito'][combined_mask]\n",
    "er_masked = channels_dict['er'][combined_mask]\n",
    "\n",
    "# --- Step 4: Calculate Masked Pearson's ---\n",
    "pcc_masked, p_value = pearsonr(mito_masked, er_masked)\n",
    "print(f\"Masked Pearson's r (Mito vs. ER): {pcc_masked:.4f}, p-value: {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673782a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Plot all pixels as a scatter plot (light alpha to avoid overplotting)\n",
    "sns.scatterplot(df_pixels, x='mito', y='er', s=1, alpha=0.1, color='gray')\n",
    "\n",
    "# Draw threshold lines to highlight quadrants\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='Mito threshold low')\n",
    "plt.axhline(threshold, color='blue', linestyle='--', label='ER threshold')\n",
    "\n",
    "# Labels and title\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75b927",
   "metadata": {},
   "source": [
    "##### --- Exercise ---\n",
    "\n",
    "Repeat the colocalization analysis steps above for a different channel pair. You can work with the created dictionaries.\n",
    "\n",
    "1. Choose another pair of channels to analyze (e.g., actin vs. er, or nuclei vs. speckles).\n",
    "\n",
    "2. Plot cytofluorogram. Create a scatter or density plot showing the pixel intensity correlation between the two channels.\n",
    "\n",
    "3. Determine appropriate thresholds for each channel (either using automatically or manually).\n",
    "\n",
    "4. Generate binary masks for both channels based on the chosen thresholds, combine them (e.g., using logical AND), and compute the masked Pearson correlation coefficient.\n",
    "\n",
    "5. Visualize the combined mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c581f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# EXAMPLE SOLUTION\n",
    "\n",
    "# Visualize pixel intensity correlation\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.histplot(df_pixels, x='actin', y='er', bins=100, cbar=True, cmap='YlGnBu_r')\n",
    "plt.show()\n",
    "\n",
    "# Create masks for significant pixels\n",
    "thresh_1 = filters.threshold_otsu(channels_dict['actin'])\n",
    "print(f\"Otsu threshold for 'ch1': {thresh_1}\")\n",
    "mask_1 = channels_dict['actin'] > thresh_1\n",
    "mask_2 = channels_dict['er'] > 2000\n",
    "\n",
    "# Combine masks\n",
    "combined_mask = mask_1 & mask_2\n",
    "ch1_masked = channels_dict['actin'][combined_mask]\n",
    "ch2_masked = channels_dict['er'][combined_mask]\n",
    "\n",
    "# Calculate masked Pearson correlation\n",
    "pcc_masked, p_value = pearsonr(ch1_masked, ch2_masked)\n",
    "print(f\"Masked Pearson's r (ch1 vs. ch2): {pcc_masked:.4f}, p-value: {p_value:.4e}\")\n",
    "\n",
    "# Visualize the combined mask\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(combined_mask, cmap='gray')\n",
    "plt.title('Combined Mask')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papi-intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
