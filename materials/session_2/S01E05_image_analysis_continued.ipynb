{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fcb5add9",
   "metadata": {},
   "source": [
    "## Image Analysis - DataFrames and Automation\n",
    "\n",
    "In this notebook, we will explore how to:\n",
    "- use **pandas** for working with tabular data\n",
    "- perform basic statistical analysis with **scipy**\n",
    "- get familiar with another Python plotting library — **seaborn**\n",
    "\n",
    "Finally, we will practice **automation on multiple files**, both for tabular data and for image processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22911561",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Lesson 1: DataFrames (pandas package)\n",
    "\n",
    "Pandas is a library used for data manipulation and analysis, particularly useful for dealing with structured data like tables. It provides the tools to create and work with DataFrames.\n",
    "\n",
    "DataFrame is a two-dimensional data structure similar to a spreadsheet (table).\n",
    "\n",
    "We typically import pandas like this:\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e4314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can create a DataFrame from different types of data.  \n",
    "\n",
    "# Create a DataFrame from a dictionary of lists\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "measurement_data = {\n",
    "    'cell_id': [1, 2, 3, 4, 5],\n",
    "    'area': [150, 230, 95, 180, 40],\n",
    "    'intensity': [88.5, 95.2, 101.0, 89.7, np.nan]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(measurement_data)\n",
    "\n",
    "# Display the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3746eb4",
   "metadata": {},
   "source": [
    "**nan** stands for **Not a Number**\n",
    "\n",
    "It’s a special floating-point value used to represent missing or undefined numerical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096893c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can easily get some statistics for our data with .describe()\n",
    "\n",
    "print(\"\\n--- Summary Statistics ---\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f412c7dd",
   "metadata": {},
   "source": [
    "#### Column/row extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b1cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can extract a single column from a pandas DataFrame by column name\n",
    "\n",
    "# This returns a pandas Series (keeps the row index)\n",
    "area_column = df['area']\n",
    "\n",
    "print(area_column) \n",
    "print()\n",
    "print(type(area_column)) # pandas Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22099a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the underlying NumPy array of the column\n",
    "# This returns raw values only (no index)\n",
    "area_values = df['area'].values\n",
    "print(area_values)\n",
    "print()\n",
    "print(type(area_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa9c17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row extraction\n",
    "\n",
    "row1 = df.iloc[0]   # try with .values\n",
    "print('First row:\\n', row1)\n",
    "\n",
    "print()\n",
    "\n",
    "row_selection = df.iloc[[0,-1]]\n",
    "print('First and last row:\\n', row_selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eccb688",
   "metadata": {},
   "source": [
    "#### Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc2706a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can filter data based on conditions\n",
    "\n",
    "# Create a new DataFrame containing only cells with area less than a value.\n",
    "filtered_df = df[df['area'] < 200]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72287d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ... results in a boolean 'mask\n",
    "less_than = df['area'] < 200\n",
    "less_than"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe6339b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we then use this mask to filter our DataFrame\n",
    "\n",
    "df[less_than]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e52073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining multiple logical conditions\n",
    "\n",
    "# Each condition must be in parentheses ( )\n",
    "filtered_df2 = df[(df['area'] < 200) & (df['area'] > 100)]\n",
    "filtered_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364c833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check in code above that indexes stay the same as in original DataFrame\n",
    "# Use reset_index if you want new indexes order\n",
    "\n",
    "filtered_df2_reset = filtered_df2.reset_index(drop=True) # check output when drop=False\n",
    "filtered_df2_reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e4f84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with missing (nan) values\n",
    "\n",
    "df_clean = df.dropna()\n",
    "df_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901dba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can sort dataframes by one or more columns.\n",
    "# ascending=True/False controls ascending/descending order\n",
    "\n",
    "sorted_df = df.sort_values(by='intensity', ascending=False)\n",
    "sorted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f8350a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first or last rows from DataFrame\n",
    "\n",
    "print(df.head(2))\n",
    "\n",
    "print(df.tail(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eea64f7",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** ---\n",
    "\n",
    "You have experimental data in a dictionary. Your task is to analyze it with Pandas.\n",
    "1. Create a **Pandas DataFrame** from the given data dictionary.\n",
    "2. Filter the DataFrame to find all rows where the **Treatment** is `'Drug_A'`.\n",
    "\n",
    "   *Hint: use the equality operator `==`*\n",
    "3. From the filtered data, compute the **average Score** for Drug A.\n",
    "\n",
    "   *Hint: you can use .mean() on extracted 'Score' column*\n",
    "4. Using the original DataFrame, find out **which treatments were `Effective`**.  \n",
    "\n",
    "   *Hint: you can use `.unique()` on the `Treatment` column, or `numpy.unique(df['Treatment'].values)` to find the unique treatment names*\n",
    "\n",
    "5. Display the **3 rows with the highest Score**.  \n",
    "\n",
    "   *Hint: you can use sort by `Score` in descending order and use `.head(3)`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59366ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## cd\n",
    "data = {'Animal_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "        'Treatment': ['Drug_A', 'Drug_B', 'Placebo', 'Drug_A', 'Drug_B', 'Placebo', 'Drug_A', 'Drug_B', 'Drug_A', 'Drug_B'],\n",
    "        'Result': ['Effective', 'Not Effective', 'Not Effective', 'Effective', 'Effective', 'Not Effective', 'Effective', 'Effective', 'Effective', 'Not Effective'],\n",
    "        'Score': [50.3, 3.5, 10.1, 17.0, 93.3, 1.5, 99.9, 73.7, 69.2, 0.5]}\n",
    "\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6810da2a",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Create DataFrame from dictionary\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 2. Filter rows where Treatment is 'Drug_A'\n",
    "drug_a_df = df[df['Treatment'] == 'Drug_A']\n",
    "print(\"Rows with Drug_A treatment:\")\n",
    "print(drug_a_df)\n",
    "\n",
    "# 3. Average Score for Drug_A\n",
    "avg_score_drug_a = drug_a_df['Score'].mean()\n",
    "print(\"\\nAverage Score for Drug_A:\", avg_score_drug_a)\n",
    "\n",
    "# 4. Which treatments were effective?\n",
    "effective_treatments = df[df['Result'] == 'Effective']['Treatment'].unique()\n",
    "print(\"\\nTreatments with at least one effective result:\", effective_treatments)\n",
    "\n",
    "# 5. Display 3 rows with the highest 'Score'\n",
    "top3_scores = df.sort_values('Score', ascending=False).head(3)\n",
    "print(\"\\nTop 3 rows by Score:\")\n",
    "print(top3_scores)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a7f948",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf64d56",
   "metadata": {},
   "source": [
    "##### Reading and saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e537fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can save our DataFrame as table (csv, excel, ...)\n",
    "sorted_df.to_csv('first_measurements.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42874ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can read tables from file into DataFrames\n",
    "loaded_df = pd.read_csv('first_measurements.csv') # check other file formats\n",
    "loaded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3356cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrames are convenient for plotting\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.scatter(loaded_df['area'], loaded_df['intensity'])\n",
    "plt.title('scatterplot')\n",
    "plt.xlabel('Area')\n",
    "plt.ylabel('Mean intensity')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01400866",
   "metadata": {},
   "source": [
    "##### Aggregation\n",
    "\n",
    "When we have multiple DataFrames, we can combine them into single one.\n",
    "\n",
    "pd.concat() is a pandas function used to combine multiple DataFrames along a particular axis (rows or columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf19bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregation (multiple tables)\n",
    "\n",
    "df_image1 = pd.read_csv(\"../data/cells_control.csv\")\n",
    "df_image2 = pd.read_csv(\"../data/cells_diseased.csv\")\n",
    "df_image3 = pd.read_csv(\"../data/cells_conditioned.csv\")\n",
    "\n",
    "# Merge dataframes vertically\n",
    "combined_df = pd.concat([df_image1, df_image2, df_image3], ignore_index=True) # default axis is 0 (rows)\n",
    "\n",
    "combined_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a812a8a",
   "metadata": {},
   "source": [
    "##### DataFrame data types\n",
    "\n",
    "Not all columns have to store numerical data.\n",
    "\n",
    "Data types can be for example:\n",
    "- int, float → numeric columns\n",
    "- bool → boolean\n",
    "- object → general-purpose column (strings, mixed data...)\n",
    "- category → categorical data (memory-efficient, useful for grouping)\n",
    "\n",
    "\n",
    "You can quickly check the data types of each column using:\n",
    "```python\n",
    "print(df.dtypes)   # Shows the type of each column\n",
    "print(df.info())   # Shows summary including data types and non-null counts\n",
    "```\n",
    "\n",
    "You can convert string/object columns to categorical with method: `.astype(\"category\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7bf8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c9ee09",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df[\"Group\"] = combined_df[\"Group\"].astype(\"category\")\n",
    "combined_df[\"State\"] = combined_df[\"State\"].astype(\"category\")\n",
    "\n",
    "print(combined_df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a506f2",
   "metadata": {},
   "source": [
    "##### Group-level statistics\n",
    "\n",
    "When your dataset has **categorical variables** (like experimental groups, treatments, or conditions) and **numeric measurements**, you might be interested in doing statistics **per group**.\n",
    "\n",
    "- `describe()` can give summary statistics for the **entire DataFrame**, but it does not separate by category\n",
    "- to get statistics per group, you can use `groupby()` combined with `agg()` (aggregate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7c3326",
   "metadata": {},
   "outputs": [],
   "source": [
    "# describe method is applied on entire dataframe without disciminating categories\n",
    "combined_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c2ca80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If we want to extract statistics per group, we can use methods group_by() and agg()\n",
    "\n",
    "# !! agg() works only on numerical columns\n",
    "# We need to remove all other types first (except the ones we are grouping by)\n",
    "reduced_df = combined_df.drop(\"State\", axis=1) # this removes column 'State'\n",
    "\n",
    "# Now we group by column 'Group'\n",
    "summary_per_group = reduced_df.groupby('Group', observed=False).agg([\"count\", \"mean\", \"std\"]) \n",
    "summary_per_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5bc8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can use multiple factors\n",
    "summary_per_group_and_state = combined_df.groupby(['Group','State'], observed=False).agg([\"count\",\"sum\", \"mean\", \"std\"]) \n",
    "summary_per_group_and_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2732471",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** ---\n",
    "\n",
    "\n",
    "1. On `combined_df`, compute **mean, maximum, and standard deviation** of all numerical columns per factor **State**. \n",
    "\n",
    "2. Find out which `State` have the largest average `Circularity`.\n",
    "\n",
    " - *Hint: you can extract values from a pivoted table by specifing column and subcolumn like [Circularity, mean].*\n",
    " - *You can use method `.idxmax()` to find the row label with the maximum value.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc08dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2643e718",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# 1. Compute mean, maximum, and std per State\n",
    "r_df = combined_df.drop('Group', axis=1)\n",
    "summary_state = r_df.groupby(\"State\", observed=False).agg([\"mean\", \"max\", \"std\"]).round(2) # optional rounding\n",
    "print(summary_state)\n",
    "\n",
    "# 2. Find out State with largest mean Circularity\n",
    "max_circ = summary_state['Circularity', 'mean'].max() # get maximum\n",
    "\n",
    "# Option 1 - # filter rows with maximum\n",
    "max_circ_df = summary_state[summary_state['Circularity', 'mean']==max_circ] # filter rows with maximum\n",
    "print(max_circ_df['State])\n",
    "\n",
    "# Option 2 - use dedicated method\n",
    "print(summary_state[\"Circularity\", \"mean\"].idxmax())\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dc4555",
   "metadata": {},
   "source": [
    "##### Visualizing group differences with Seaborn\n",
    "\n",
    "The **Seaborn** library builds on Matplotlib but provides higher-level plotting functions tailored for statistical data.\n",
    "\n",
    "A particularly useful feature is the `hue` argument, which colors data points by group/category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0227b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(\n",
    "    data=combined_df, \n",
    "    x=\"Group\", \n",
    "    y=\"Mean_Intensity\", \n",
    "    hue=\"Group\", \n",
    "    palette=\"Set2\" # color-coding\n",
    "    )\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f956db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some seaborn plots also allow discrimination of multiple factors\n",
    "  \n",
    "plt.figure(figsize=(6, 6))\n",
    "sns.scatterplot(\n",
    "    data=combined_df, \n",
    "    x=\"Area\", y=\"Circularity\", \n",
    "    hue=\"Group\", style=\"State\"\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a86049",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing\n",
    "\n",
    "Visualizations help us see patterns and differences in data, but they are not enough for statistical confirmation.  \n",
    "To formally test whether groups differ, we use hypothesis testing.\n",
    "\n",
    "In Python, the `scipy.stats` module provides a variety of statistical tests.\n",
    "\n",
    "Below, we will use a **t-test** to compare whether the **means of two groups are equal**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6212f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis testing\n",
    "\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# the following is combination of filtering DataFrame and extracting column Mean_Intensity\n",
    "controls = combined_df[combined_df['Group']=='Control']['Mean_Intensity']\n",
    "diseased = combined_df[combined_df['Group']=='Diseased']['Mean_Intensity']\n",
    "\n",
    "# the t-test takes two lists/arrays \n",
    "# by default the ttest_ind() performs two-sided t-test and assumes equal variance\n",
    "t_stat, p_value = ttest_ind(controls, diseased)\n",
    "\n",
    "print(f\"\\nT-test p-value for comparing areas: {p_value:.9f}\") # formatting number to 9 decimal places"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca48a6",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** ---\n",
    "\n",
    "1. Make a boxplot comparing **Area** across categories of **State**.\n",
    "2. Add a swarmplot (`sns.swarmplot`) on top of boxplot to see individual datapoints.\n",
    "3. Perform a t-test comparing \"Circularity\" between Control and Diseased.\n",
    "4. Perform a t-test comparing \"Mean_Intensity\" between Starved Pre-conditioned and Unstarved Pre-conditioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eced295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37b2422",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# Boxplot with swarmplot for Area per State\n",
    "plt.figure(figsize=(6, 4))\n",
    "sns.boxplot(data=combined_df, x=\"State\", y=\"Area\", palette=\"Set2\", hue='State')\n",
    "sns.swarmplot(data=combined_df, x=\"State\", y=\"Area\", color='black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# T-test Circularity: Control vs Diseased\n",
    "controls = combined_df[combined_df[\"Group\"] == \"Control\"][\"Circularity\"]\n",
    "diseased = combined_df[combined_df[\"Group\"] == \"Diseased\"][\"Circularity\"]\n",
    "t_stat, p_value = ttest_ind(controls, diseased)\n",
    "print(f\"T-test Circularity (Control vs Diseased): p = {p_value:.4f}\")\n",
    "\n",
    "# T-test Mean_Intensity: Starved vs Unstarved Pre-conditioned\n",
    "pre_starved = combined_df[\n",
    "    (combined_df[\"Group\"] == \"Pre-conditioned\") & (combined_df[\"State\"] == \"Starved\")\n",
    "][\"Mean_Intensity\"]\n",
    "pre_unstarved = combined_df[\n",
    "    (combined_df[\"Group\"] == \"Pre-conditioned\") & (combined_df[\"State\"] == \"Unstarved\")\n",
    "][\"Mean_Intensity\"]\n",
    "t_stat, p_value = ttest_ind(pre_starved, pre_unstarved)\n",
    "print(f\"T-test Mean_Intensity (Starved vs Unstarved, Pre-conditioned): p = {p_value:.4f}\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e1298b",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Lesson 2: Automation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc64e28d",
   "metadata": {},
   "source": [
    "#### A Quick Guide to File Paths\n",
    "\n",
    "Python offers several ways to handle paths, each with its own strengths. All are usefull as they automatically use the correct path separator (`/` or `\\`).\n",
    "\n",
    "Covered packages: `os`, `pathlib`, and `glob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8447d3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- os.path.join(): The safe way to build paths ---\n",
    "import os\n",
    "\n",
    "# This automatically uses the correct separator for your OS (`/` or `\\`)\n",
    "folder = \"data\"\n",
    "filename = \"image_01.tif\"\n",
    "\n",
    "correct_path = os.path.join(folder, filename)\n",
    "print(f\"OS-safe path: {correct_path}\")\n",
    "\n",
    "correct_path = os.path.join(folder, 'subfolder', filename)\n",
    "print(f\"OS-safe path: {correct_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570b6dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directory path and filename parts\n",
    "base_name = os.path.basename(correct_path) # image_01.tif\n",
    "parent_path = os.path.dirname(correct_path) # data\\subfolder\n",
    "parent_dir = os.path.basename(parent_path) # subfolder\n",
    "\n",
    "print(f\"File name part: {base_name}\")\n",
    "print(f\"Parent directory path: {parent_path}\")\n",
    "print(f\"Parent directory name: {parent_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fb8f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- pathlib  ---\n",
    "from pathlib import Path\n",
    "\n",
    "# pathlib treats paths as objects, you can use the `/` operator to build paths\n",
    "#csv_file_path = '..' / 'data' / \"cells_control.csv\" # this gives error\n",
    "csv_file_path = '..' / Path('data') / \"cells_control.csv\"\n",
    "print(f\"Path:   {csv_file_path}   is   {type(csv_file_path)}\")\n",
    "\n",
    "print(f\"Does this file exist? {csv_file_path.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c841924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Get parts of the path ---\n",
    "\n",
    "print(f\"Filename only: {csv_file_path.name}\")\n",
    "print(f\"Parent folder path: {csv_file_path.parent}\")\n",
    "print(f\"Parent folder: {csv_file_path.parent.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557aa314",
   "metadata": {},
   "source": [
    "Creating new directories (folders) is possible with both os and pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b0b876",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the directory if it doesn’t exist yet\n",
    "\n",
    "# os\n",
    "os.makedirs('test_folder_os', exist_ok=True)\n",
    "\n",
    "# pathlib\n",
    "folder = Path(\"test_folder_pathlib\")\n",
    "folder.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938f30db",
   "metadata": {},
   "source": [
    "##### Listing files in folder\n",
    "\n",
    "**os**\n",
    "```python\n",
    "os.listdir()\n",
    "```\n",
    "\n",
    "**glob** - finding files with wildcards \n",
    "```python\n",
    "import glob\n",
    "glob.glob(folder/*.csv) \n",
    "\n",
    "# or \n",
    "Path(\"folder\").glob(\"*.csv\") # returns a generator, not a list\n",
    "```\n",
    "\n",
    "\n",
    "Wildcards are special characters that help you match patterns in file names — instead of typing exact file names.\n",
    "\n",
    "For example: * means any number of characters\n",
    "\n",
    "| Goal                | glob                                         | pathlib                       |\n",
    "| ------------------- | -------------------------------------------- | ----------------------------- |\n",
    "| Find only in folder | `glob.glob(\"folder/*.tif\")`                    | `Path(\"folder\").glob(\"*.tif\")`  |\n",
    "| Find recursively    | `glob.glob(\"folder/**/*.tif\", recursive=True)` | `Path(\"folder\").rglob(\"*.tif\")` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae75b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir() lists all files/folders\n",
    "\n",
    "my_path = os.path.join('..', 'data')\n",
    "files = os.listdir(my_path)\n",
    "\n",
    "# Output is a list of filenames\n",
    "print(files[:10]) # print first 10 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2648ef35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List comprehension\n",
    "only_tif_files = [tif for tif in files if tif.endswith('.tif')]\n",
    "\n",
    "# the above is same as typing:\n",
    "only_tif_files = []\n",
    "for tif in files:\n",
    "    if tif.endswith('.tif'):\n",
    "        only_tif_files.append(tif)\n",
    "\n",
    "# print selection\n",
    "print(only_tif_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83dc70f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- glob.glob(): the way to find files with wildcards ---\n",
    "import glob\n",
    "\n",
    "# Find all files ending with .tif\n",
    "my_path = os.path.join('..', 'data')\n",
    "\n",
    "tif_files = glob.glob(os.path.join(my_path, \"*.tif\"))\n",
    "\n",
    "print(f\"Found n TIF files: {len(tif_files)}\")\n",
    "print(tif_files[-5:]) # glob returns list of strings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b529c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To search recursively (including subfolders)\n",
    "\n",
    "tif_files_recursive = glob.glob(os.path.join(r\"../data\", \"**\", \"*.tif\"), recursive=True)\n",
    "print(len(tif_files_recursive))\n",
    "print(tif_files_recursive[-5:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc540c",
   "metadata": {},
   "source": [
    "##### --- **Exercise** ---\n",
    "\n",
    "1. **Create a path** to the `data` folder by combining the relative marker `'..'` and the folder name `'data'` - store the path in a variable `data_path`.\n",
    "   - Use either **`os`** or **`pathlib`**.\n",
    "2. **List all files** in your defined path and print how many files were found (`len()` function).  \n",
    "    - *Hint*: Use `os.listdir()` or `glob.glob(data_path+\"/*\")`\n",
    "3. **List all images** in the folder (ending with **.tif** or **.png**).\n",
    "    - *Hint*: can filter the previous output with `endswith()` function \n",
    "    - or combine glob searches like `glob.glob(\"folder/*.tif\") + glob.glob(\"folder/*.png\")`\n",
    "4. **Find all .csv files** that include **'con'** in their name (e.g. control, conditioned)\n",
    "    - *Hint*: use `'substring' in 'string'` to find if `'con'` is in the filename\n",
    "    - Read them into a list of DataFrames with pandas - `pd.read_csv()`\n",
    "    - Combine list of DataFrames into one DataFrame - `pd.concat(list_of_dfs, ignore_index=True)`\n",
    "    - Save the combined DataFrame to a new folder `results` as `combined.csv` \n",
    "        - `os.makedirs()` to create a folder and `dataframe.to_csv()` to save DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa38303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee6a0bc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "\n",
    "```python\n",
    "# EXAMPLE SOLUTION\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- 1. Create path to data folder ---\n",
    "data_path = os.path.join(\"..\", \"data\")\n",
    "print(\"Data path:\", data_path)\n",
    "# or\n",
    "data_path2 = Path(\"..\") / \"data\"\n",
    "print(\"Data path:\", data_path2)\n",
    "\n",
    "# --- 2. List all files and count them ---\n",
    "all_files = glob.glob(os.path.join(data_path, \"*\"))\n",
    "# or\n",
    "all_files = glob.glob(data_path+\"/*\"))\n",
    "print(f\"Found {len(all_files)} files:\")\n",
    "print(all_files)\n",
    "\n",
    "# --- 3. List all image files (.tif or .png) ---\n",
    "image_files = glob.glob(os.path.join(data_path, \"*.tif\")) + glob.glob(os.path.join(data_path, \"*.png\"))\n",
    "print(f\"\\nFound {len(image_files)} image files:\")\n",
    "print(image_files)\n",
    "\n",
    "# or list comprehension with tuple ('.tif', '.png')\n",
    "image_files2 = [file for file in all_files if file.endswith(('.tif', '.png'))]\n",
    "print(f\"\\nFound {len(image_files2)} image files.\")\n",
    "\n",
    "# --- 4. Find all CSVs with 'con' in name ---\n",
    "csv_files = glob.glob(os.path.join(data_path, \"*.csv\"))\n",
    "csv_files_con = [f for f in csv_files if \"con\" in os.path.basename(f)]\n",
    "print(f\"\\nCSV files with 'con' in name: {csv_files_con}\")\n",
    "\n",
    "# read the CSVs with pandas and store them in a list\n",
    "dfs = [pd.read_csv(f) for f in csv_files_con]\n",
    "\n",
    "# merge Dataframes into a single one\n",
    "combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Create results folder and save the final DataFrame\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "combined_df.to_csv(\"results/combined.csv\", index=False)\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be987da",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "### Lesson 3: Image analysis workflow - recap\n",
    "\n",
    "In the introductury session, we practiced designing and tuning a full image-processing workflow:\n",
    "- We loaded and visualized microscopy images.\n",
    "- We adjusted individual processing steps such as filtering, thresholding, and segmentation.\n",
    "- We extracted quantitative measurements (e.g., area, intensity, circularity).\n",
    "- We packaged this workflow in a function to be able to run it in a loop.\n",
    "\n",
    "- Now, we will recap this session\n",
    "- Add analysis of measurements using **Pandas**, **Seaborn**, and **scipy** packages.\n",
    "- Practice automatic processing of batch of images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2da35d8",
   "metadata": {},
   "source": [
    "##### Image loading\n",
    "\n",
    "Example image loading libraries:\n",
    "- scikit-image (skimage)\n",
    "- imageio\n",
    "- bioio\n",
    "- tifffile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211c26d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "import imageio.v3 as iio\n",
    "\n",
    "input_image = io.imread('../data/noisy_cells.tif')\n",
    "input_image2 = iio.imread('../data/noisy_cells.tif')\n",
    "\n",
    "print('Image shape:', input_image.shape)\n",
    "print('Image shape:', input_image2.shape)\n",
    "\n",
    "print('Are images identical?', np.array_equal(input_image, input_image2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddd032f",
   "metadata": {},
   "source": [
    "##### Image processing & segmentation\n",
    "\n",
    "Libraries like skimage and scipy provide a variety of modules with functions for common image-processing operations — such as filtering, thresholding, transformations, and other manipulations of images or masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce2d037",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import filters\n",
    "\n",
    "gaussian_blur = filters.gaussian(input_image, sigma=2)         \n",
    "# Smooths image, reduces noise by averaging neighboring pixels\n",
    "\n",
    "median_filtered = filters.median(input_image)                  \n",
    "# Reduces noise while better preserving edges\n",
    "\n",
    "sobel_edges = filters.sobel(input_image)                       \n",
    "# Enhances edges by computing the gradient magnitude of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8597fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(8, 8))\n",
    "ax = axes.ravel()\n",
    "\n",
    "ax[0].imshow(input_image, cmap='gray')\n",
    "ax[0].set_title('Original Image')\n",
    "\n",
    "ax[1].imshow(gaussian_blur, cmap='gray')\n",
    "ax[1].set_title('Gaussian Blur')\n",
    "\n",
    "ax[2].imshow(median_filtered, cmap='gray')\n",
    "ax[2].set_title('Median Filter')\n",
    "\n",
    "ax[3].imshow(sobel_edges, cmap='gray')\n",
    "ax[3].set_title('Sobel Edges')\n",
    "\n",
    "\n",
    "for a in ax:\n",
    "    a.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58997933",
   "metadata": {},
   "source": [
    "Segmentation is the process of separating an image into regions. \n",
    "\n",
    "In a simple case, we can segment image pixels into two classes (e.g., foreground vs. background) by applying a threshold on pixel values.\n",
    "\n",
    "The thresholded image produces a mask, which is a boolean array indicating which pixels belong to the foreground (True) and which to the background (False).\n",
    "\n",
    "\n",
    "The result of instance segmentation using `skimage.measure.label()` is called labels. Labels is typically an integer array where each connected object (e.g., a cell, a nucleus) is assigned a unique integer value, and the background is usually 0. This allows to identify and analyze individual objects separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaeed6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_m = 30 # manual \n",
    "threshold_a = filters.threshold_mean(gaussian_blur) # automatic \n",
    "\n",
    "print('Detected threshold value:', threshold_a)\n",
    "\n",
    "binary_mask = gaussian_blur > threshold_a\n",
    "print(binary_mask.dtype)\n",
    "\n",
    "plt.imshow(binary_mask, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445957df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improve mask with post-processing\n",
    "from skimage import morphology\n",
    "filled = morphology.remove_small_holes(binary_mask, area_threshold=200)\n",
    "cleaned = morphology.remove_small_objects(filled, min_size=50)\n",
    "\n",
    "plt.imshow(cleaned, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed4d536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure\n",
    "\n",
    "labels = measure.label(cleaned)\n",
    "print(labels.dtype)\n",
    "\n",
    "plt.imshow(labels, cmap = 'nipy_spectral')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce44e471",
   "metadata": {},
   "source": [
    "##### Object measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3186f5",
   "metadata": {},
   "source": [
    "The `measure.regionprops_table()` function from scikit-image computes quantitative features for each labeled region in an image.  \n",
    "\n",
    "Each unique region (identified by a unique label) can be characterized by geometric and intensity-based properties.\n",
    "\n",
    "The output of this function is a dictionary containing one array per measured property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defc57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Measurements\n",
    " \n",
    "# We specify which properties we want to measure for each object.\n",
    "properties_to_measure = ['label', 'area', 'mean_intensity', 'perimeter', 'eccentricity']\n",
    "\n",
    "# regionprops_table uses our `labels` image and the original `nuclei_image`\n",
    "props_dict = measure.regionprops_table(\n",
    "    labels, # input mask/labels image\n",
    "    intensity_image=input_image, # for raw intensity measurements input intensity image\n",
    "    properties=properties_to_measure # define an interable (list, tuple, set..) of properties to measure\n",
    ")\n",
    "\n",
    "props_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5d7fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the dictionary of results into a Pandas DataFrame\n",
    "import pandas as pd\n",
    "\n",
    "image_df = pd.DataFrame(props_dict)\n",
    "\n",
    "image_df.head(5) # show first 5 rows of a table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5ff40",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Process all images in folder\n",
    "\n",
    "Let's combine everything together.\n",
    "\n",
    "We will use a for-loop to apply a user-defined function (containing image-processing and measurement steps) to all images in a folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae65055b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Define our reusable analysis function\n",
    "def analyze_image(image_array, image_name):\n",
    "    blurred_image = filters.gaussian(image_array, sigma=3)\n",
    "    threshold_value = filters.threshold_otsu(blurred_image)\n",
    "    mask = blurred_image > threshold_value\n",
    "    label_image = measure.label(mask)\n",
    "    props_dict = measure.regionprops_table(label_image, intensity_image=image_array,\n",
    "                                           properties=('label', 'area', 'mean_intensity'))\n",
    "    results_df = pd.DataFrame(props_dict)\n",
    "    # Add the image name to track our data\n",
    "    results_df['source_image'] = image_name\n",
    "\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fb2eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Find the files\n",
    "input_folder = r\"../data/batch_analysis/input\"\n",
    "file_list = glob.glob(os.path.join(input_folder, \"*.png\"))\n",
    "\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd93fd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Loop and aggregate results\n",
    "import imageio.v3 as iio\n",
    "\n",
    "all_image_results = []\n",
    "print(\"\\nStarting batch processing...\")\n",
    "\n",
    "# Iterate over list of filepaths\n",
    "for file_path in file_list:\n",
    "    print(f\"Processing: {file_path}\")\n",
    "    \n",
    "    # Read image\n",
    "    image = iio.imread(file_path)\n",
    "    \n",
    "    # Get just the filename from the full path for cleaner tables\n",
    "    filename_only = os.path.basename(file_path)\n",
    "    \n",
    "    # Apply our function - our function returns DataFrame with measurements\n",
    "    single_image_df = analyze_image(image, filename_only)\n",
    "\n",
    "    # Add DataFrame for currently processed file to a pre-defined list\n",
    "    all_image_results.append(single_image_df)\n",
    "    \n",
    "print()\n",
    "print('Number of dataframes in list:', len(all_image_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d096fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Concatenate into a final DataFrame\n",
    "final_batch_df = pd.concat(all_image_results, ignore_index=True)\n",
    "\n",
    "print(\"\\n--- Final Batch Results ---\")\n",
    "final_batch_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f89f54",
   "metadata": {},
   "source": [
    "##### --- ***Exercise*** --- \n",
    "\n",
    "Work with provided `analyze_nuclei` function (from introductory session) to solve the following exercise.\n",
    "\n",
    "The function takes in 2 arguments - a microscopic image (2D array) and its name (string). \n",
    "\n",
    "The function should identify individual nuclei, measure their properties (e.g., area and mean intensity), and return both a labeled image and a DataFrame containing the measurements.\n",
    "\n",
    "***Batch Process the Entire Folder***\n",
    "\n",
    "You know the tools, you have the function. Now it's time to automate everything!\n",
    "\n",
    "**Instructions:**\n",
    "1. Use glob to get a list of paths to all `.tif` files in the `data/batch_analysis/nuclei_data` folder.\n",
    "2. Create an empty list called `all_nuclei_results`.\n",
    "3. Write a `for` loop that iterates through your list of file paths. \n",
    "- Inside the loop:\n",
    "    - Load the current image using `iio.imread()`.\n",
    "    - Get just the filename (without the folder path) - e.g. using `os.path.basename()`.\n",
    "    - Call your `analyze_nuclei` function with the loaded image and the filename.\n",
    "    - Append the DataFrame returned by the function to your `all_nuclei_results` list.\n",
    "4. After the loop, use `pd.concat()` to combine the list of DataFrames into a single, master DataFrame.\n",
    "5. Print the head and tail of your final DataFrame to see the combined results from all images.\n",
    "6. Find out how many nuclei per image were detected and print it. \n",
    "    - *Hint: you can use .groupby('filename').describe()*\n",
    "7. As a final analysis, create a boxplot showing the distribution of nuclei `area` for each of the images.\n",
    "    - *Hint*: `seaborn.boxplot` is great for this, you can use `filename` column as `hue` argument. \n",
    "    - *Usage: `sns.boxplot(data=dataframe, x=x_axis_column, y=y_axis_column, hue=group_column)`*\n",
    "\n",
    "8. Bonus for fast solvers: write a new function to minimize the code needed for solving instructions - (skip instructions 5 and 6) - ideally, your function should take only path as input and return a boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27037352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this code cell\n",
    "\n",
    "from skimage import filters, morphology, measure\n",
    "import scipy.ndimage as ndi\n",
    "\n",
    "def analyze_nuclei(image_array, file_name):\n",
    "    filtered_array = filters.gaussian(image_array, sigma=1)\n",
    "    threshold_value = filters.threshold_otsu(filtered_array)\n",
    "    mask = filtered_array > threshold_value\n",
    "    processed_mask = ndi.binary_fill_holes(mask)\n",
    "    mask_cleaned = morphology.remove_small_objects(processed_mask, min_size=50)\n",
    "    label_image = measure.label(mask_cleaned)\n",
    "\n",
    "    props = measure.regionprops_table(label_image, intensity_image=image_array,\n",
    "                                      properties=['label', 'area', 'mean_intensity'])\n",
    "    df = pd.DataFrame(props)\n",
    "\n",
    "    df['filename'] = file_name\n",
    "\n",
    "    return df, label_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884ef074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376df0c3",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# 1. Get list of all .tif files in the folder\n",
    "image_files = glob.glob(\"../data/batch_analysis/nuclei_data/*.tif\")\n",
    "\n",
    "# 2. Create an empty list to store results\n",
    "all_nuclei_results = []\n",
    "\n",
    "# 3. Loop through each file\n",
    "for file_path in image_files:\n",
    "    # Load image\n",
    "    image_array = iio.imread(file_path)\n",
    "    \n",
    "    # Extract filename only\n",
    "    filename = os.path.basename(file_path)\n",
    "    \n",
    "    # Analyze nuclei\n",
    "    df, labels = analyze_nuclei(image_array, filename)\n",
    "    \n",
    "    # Append results to list\n",
    "    all_nuclei_results.append(df)\n",
    "\n",
    "# 4. Combine all DataFrames into a single master DataFrame\n",
    "master_df = pd.concat(all_nuclei_results, ignore_index=True)\n",
    "\n",
    "# 5. Inspect results\n",
    "print(master_df.head())\n",
    "print(master_df.tail())\n",
    "\n",
    "# 6. Print counts per image\n",
    "print(master_df.groupby('filename').describe())\n",
    "\n",
    "# 7. Boxplot of nuclei area per image\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(data=master_df, x='filename', y='area', palette=\"Set3\", hue='filename')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel(\"Nuclei Area\")\n",
    "plt.xlabel(\"Source Image\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b4f953",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution for bonus</summary>\n",
    "\n",
    "```python\n",
    "# Example solution for bonus\n",
    "def plot_nuclei_per_image(folder_path):\n",
    "\n",
    "    image_files = glob.glob(folder_path+\"/*.tif\")\n",
    "\n",
    "    dfs = []\n",
    "\n",
    "    for image_file in image_files:\n",
    "        \n",
    "        image_array = iio.imread(image_file)\n",
    "        image_name = os.path.basename(image_file)\n",
    "\n",
    "        df, _ = analyze_nuclei(image_array, image_name)\n",
    "        dfs.append(df)\n",
    "\n",
    "    master_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    sns.boxplot(data=master_df, x='filename', y='area', palette=\"Set3\", hue='filename')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.ylabel(\"Nuclei Area\")\n",
    "    plt.xlabel(\"Source Image\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Calling function on given path\n",
    "images_path = '../data/batch_analysis/nuclei_data'\n",
    "plot_nuclei_per_image(images_path)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9558e98d",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "#### Slice-by slice reading\n",
    "\n",
    "When working with image data stored as a series of files, it is often useful to load all slices into a single multi-dimensional array for easier processing.\n",
    "\n",
    "The **scikit-image** function `imread_collection()` allows you to read multiple images at once using a filename pattern.\n",
    "You can use a wildcard (*) to match all files in a folder that belong to your dataset. This collection can then be converted into a **NumPy** array, effectively creating an image stack.\n",
    "\n",
    "However, the number of z-slices, channels or frames is not recognized. You have to reshape the loaded data into the appropriate multi-dimensional form (for example, ZCYX) yourself.\n",
    "\n",
    "*Note*: Alternatively, you can build your own for-loops to load images from disk manually.\n",
    "This approach gives you more flexibility — for example, to sort slices and channels, skip specific files, or arrange the data into custom dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98eb191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import io\n",
    "\n",
    "im_collection = io.imread_collection('../data/batch_analysis/tiffs/' + \"*\")\n",
    "image_stack = im_collection.concatenate()\n",
    "image_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4c8474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the shape of array with reshape\n",
    "\n",
    "num_channels = 2\n",
    "num_z_slices = 5\n",
    "num_t_frames = 10\n",
    "image5d = np.reshape(image_stack, (num_t_frames, num_z_slices, num_channels, image_stack.shape[-2], image_stack.shape[-1]))\n",
    "image5d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d428a7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use stackview for interactive multi-dimensional image plot \n",
    "import stackview\n",
    "stackview.slice(image5d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6a4b9",
   "metadata": {},
   "source": [
    "### USE CASE - Pixel-based colocalization \n",
    "\n",
    "Pixel-based colocalization assesses the 'overlap' between two or more channels. (Colocalization only indicates that two signals are present within the same volume resolved by the microscope - it does not prove molecular interaction.)\n",
    "\n",
    "There are many types of coefficients to characterize colocalization (Pearson’s correlation, Spearman’s rank correlation, Manders’ overlap coefficients, intersection coefficients, cross-correlation analysis...). In this notebook we will compute the Pearson's correlation coefficient.\n",
    "\n",
    "- **Pearson's Correlation Coefficient (PCC):** Measures the linear relationship between the two channels' intensities. It ranges from -1 (perfect negative correlation) to +1 (perfect possitive correlation), with 0 indicating no correlation. A major drawback is its sensitivity to background pixels, which often creates a strong, artificial positive correlation. Thresholding is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "732b1831",
   "metadata": {},
   "source": [
    "##### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfa9095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup imports and \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import stackview\n",
    "from skimage.io import imread\n",
    "from skimage import filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1dacf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a multi-channel image\n",
    "\n",
    "raw_image = imread(r'../data/cellpainting.tif')\n",
    "\n",
    "print(raw_image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4875d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use stackview for interactive visualization of the channels\n",
    "\n",
    "stackview.switch(\n",
    "    {\"actin\":   raw_image[0,:,:],\n",
    "     \"er\":raw_image[1,:,:],\n",
    "     \"speckles\":      raw_image[2,:,:],\n",
    "     \"mito\":      raw_image[3,:,:],\n",
    "     \"nuclei\":      raw_image[4,:,:],\n",
    "    },\n",
    "    colormap=['pure_red',\"pure_green\", \"pure_yellow\", \"pure_magenta\", 'pure_cyan'],\n",
    "    toggleable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf47ff96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store channels in a dictionary for easy access\n",
    "\n",
    "channels_dict = {\n",
    "    \"actin\": raw_image[0,:,:],\n",
    "    \"er\": raw_image[1,:,:],\n",
    "    \"speckles\": raw_image[2,:,:],\n",
    "    \"mito\": raw_image[3,:,:],\n",
    "    \"nuclei\": raw_image[4,:,:],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b688cbb2",
   "metadata": {},
   "source": [
    "Visualize pixel intensities with a cytofluorogram.\n",
    "\n",
    "**The Cytofluorogram:**\n",
    "\n",
    "A 2D scatter plot for visualization of pixel-wise intensity relationship between two image channels. Each point on the plot represents a single pixel, with its intensity in Channel 1 on x-axis and its intensity in Channel 2 on y-axis.\n",
    "\n",
    "Let's create a cytofluorogram for the **mito** and **er** channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aedd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For histogram ploting we will flatten the 2D image arrays into 1D arrays of pixels intensities\n",
    "\n",
    "# Again, we will do it for all channels and create a new dictionary to store the results\n",
    "f_channels_dict = {key: value.ravel() for key, value in channels_dict.items()}\n",
    "\n",
    "for key, arr in f_channels_dict.items():\n",
    "    print(key, arr.shape)\n",
    "\n",
    "# We can also create a pandas DataFrame for convenience\n",
    "channels_df = pd.DataFrame(f_channels_dict)\n",
    "channels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45282eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- The Cytofluorogram (Scatter plot) ---\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x=channels_df['mito'], y=channels_df['er'], s=1, alpha=0.1)\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.xlabel(\"Mitochondria intensity\")\n",
    "plt.ylabel(\"ER intensity\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b00d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A 2D histogram is often more informative for dense data\n",
    "# Note the high density of points near the origin, representing background pixels.\n",
    "\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.histplot(channels_df, x='mito', y='er', cbar=True, cmap='YlGnBu_r', bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a5f79a",
   "metadata": {},
   "source": [
    "Calculating Pearson's Correlation Coefficient (PCC)\n",
    "\n",
    "First, we calculate PCC on the raw pixel data, which includes a vast number of dark background pixels. This will demonstrate how background skews the result.\n",
    "\n",
    "We can use `scipy` library to compute PCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763f97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Pearson's Correlation ---\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "pcc, p_value = pearsonr(channels_df['mito'], channels_df['er'])\n",
    "print(f\"Pearson's r (Mito vs. ER): {pcc:.4f}, p-value: {p_value:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca691b2c",
   "metadata": {},
   "source": [
    "Now, we'll apply a threshold to each channel to create mask that separates signal from background. \n",
    "\n",
    "By analyzing only the pixels within these masks, we get a more biologically meaningful correlation value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2effcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 1: Create a mask for significant pixels in each channel ---\n",
    "\n",
    "# We can use Otsu's method to automatically find a threshold\n",
    "thresh_er = filters.threshold_otsu(channels_dict['er'])\n",
    "print(f\"Otsu threshold for 'er': {thresh_er}\")\n",
    "\n",
    "# Or we might use a manual threshold based on visual inspection\n",
    "threshold = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c9ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can now simply filter the DataFrame\n",
    "# Note: for simplicity, we are setting a single threshold value for both channels \n",
    "# this ignores differences in intensity distributions\n",
    "new_df = channels_df[(channels_df['mito']>threshold) & (channels_df['er']>threshold)]\n",
    "\n",
    "print(f'Remaining pixels: {len(new_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5f825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute PCC on filtered data\n",
    "\n",
    "pcc, p_value = pearsonr(new_df['mito'], new_df['er'])\n",
    "print(f\"Pearson's r (Mito vs. ER): {pcc:.4f}, p-value: {p_value:.4e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14c6381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new figure\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# Plot histograms/scatter data\n",
    "sns.histplot(channels_df, x='mito', y='er', color='gray', bins=100)\n",
    "sns.histplot(new_df, x='mito', y='er', cmap='YlGnBu_r', bins=100)\n",
    "\n",
    "# Draw threshold lines to highlight quadrants\n",
    "plt.axvline(threshold, color='red', linestyle='--', label='Mito threshold low')\n",
    "plt.axhline(threshold, color='blue', linestyle='--', label='ER threshold')\n",
    "\n",
    "# Labels and title\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44535146",
   "metadata": {},
   "source": [
    "Lets visualize our mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc403c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply threshold on 2D array data\n",
    "mask_er = channels_dict['er'] > threshold\n",
    "mask_mito = channels_dict['mito'] > threshold\n",
    "\n",
    "# Combine masks using logical operation\n",
    "combined_mask = np.logical_and(mask_mito, mask_er)\n",
    "# or\n",
    "combined_mask = mask_mito & mask_er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f7e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize all masks side-by-side\n",
    "\n",
    "# subplots allow us to show grid of plots in one figure \n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n",
    "\n",
    "axes[0].imshow(mask_mito, cmap='gray')\n",
    "axes[0].set_title('Mito Mask')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(mask_er, cmap='gray')\n",
    "axes[1].set_title('ER Mask')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(combined_mask, cmap='gray')\n",
    "axes[2].set_title('Combined Mask')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec4698",
   "metadata": {},
   "source": [
    "##### Recap: Operations between masks\n",
    "\n",
    "In image analysis, it is often useful to combine or compare different binary masks to extract specific regions of interest. \n",
    "\n",
    "Masks as boolean arrays (True for foreground, False for background), can be processed with logical operations to create new masks.\n",
    "\n",
    "Common operations:\n",
    "- *Inversion - NOT (`~`)* – flips all boolean values:\n",
    "    ```python\n",
    "    inverted_mask = ~mask\n",
    "    ```\n",
    "- *Intersection (`&`)* – keeps only pixels present in both masks:\n",
    "    ```python\n",
    "    overlap_mask = mask1 & mask2\n",
    "    ```\n",
    "- *Union (`|`)* – includes all pixels present in either mask:\n",
    "    ```python\n",
    "    combined_mask = mask1 | mask2\n",
    "    ```\n",
    "- *Difference / Subtraction (`& ~`)* – removes pixels of one mask from another:\n",
    "    ```python\n",
    "    cytoplasm_mask = cell_mask & ~nuclei_mask\n",
    "    ```\n",
    "- *Exclusive OR (`^`)* – pixels present in one mask or the other, but not both:\n",
    "    ```python\n",
    "    xor_mask = mask1 ^ mask2\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb75b927",
   "metadata": {},
   "source": [
    "##### --- Exercise ---\n",
    "\n",
    "Repeat the colocalization analysis steps above for a different channel pair. You can work with the created dictionaries.\n",
    "\n",
    "1. Choose another pair of channels to analyze (e.g., actin vs. er, or nuclei vs. speckles).\n",
    "\n",
    "2. Plot cytofluorogram. Create a scatter or histogram plot showing the pixel intensity correlation between the two channels.\n",
    "\n",
    "3. Determine appropriate thresholds for each channel (either manually or using automatic methods).\n",
    "\n",
    "4. Generate binary masks for both channels based on the chosen thresholds, combine them (e.g., using logical AND), and compute the masked Pearson correlation coefficient inside masked area.\n",
    "\n",
    "5. Visualize the combined mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b5f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594c581f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Click to see the example solution</summary>\n",
    "\n",
    "```python\n",
    "# EXAMPLE SOLUTION\n",
    "\n",
    "# Visualize pixel intensity correlation\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.histplot(channels_df, x='actin', y='er', bins=100, cbar=True, cmap='YlGnBu_r')\n",
    "plt.show()\n",
    "\n",
    "# Create masks for significant pixels\n",
    "thresh_1 = filters.threshold_otsu(channels_dict['actin'])\n",
    "print(f\"Otsu threshold for 'ch1': {thresh_1}\")\n",
    "mask_1 = channels_dict['actin'] > thresh_1\n",
    "\n",
    "thresh_2 = 4000\n",
    "mask_2 = channels_dict['er'] > thresh_2\n",
    "\n",
    "# Combine masks\n",
    "combined_mask = mask_1 & mask_2\n",
    "\n",
    "# Apply masks \n",
    "masked_df = channels_df[(channels_df['actin']>thresh_1) & (channels_df['er']>thresh_2)]\n",
    "\n",
    "# Calculate masked Pearson correlation\n",
    "pcc_masked, p_value = pearsonr(masked_df['actin'], masked_df['er'])\n",
    "print(f\"Masked Pearson's r (ch1 vs. ch2): {pcc_masked:.4f}, p-value: {p_value:.4e}\")\n",
    "\n",
    "# Visualize the combined mask\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(combined_mask, cmap='gray')\n",
    "plt.title('Combined Mask')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "papi-advanced",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
